{"example_idx": 154, "doc_key": "1909.07575", "question": "What are the baselines?", "overlap_recall": 0.041666666666666664, "baseline_ctx_tokens": 1928, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [91, 80, 78, 70, 65, 72, 66, 68, 26, 40, 71, 81, 73, 74, 86, 79, 60, 59, 11, 32, 67, 31, 61, 88], "raptor_node_indices": [9, 12, 8, 19, 5, 17, 6, 20, 15, 4, 2, 16, 10, 18, 7, 14, 13, 21, 3, 0, 1, 11], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" To alleviate these issues, we have proposed a method, which is capable of reusing every sub-net and keeping the role of sub-net consistent between pre-training and fine-tuning  Empirical studies have demonstrated that our model significantly outperforms baselines", " It indicates that the gain comes from bridging the gap between pre-training and fine-tuning rather than a better fine-tuning process Experiments ::: Discussion ::: Compared with a Cascaded System Table TABREF29 compares our model with end-to-end baselines  Here, we compare our model with cascaded systems  We build a cascaded system by combining the ASR model and MT model used in pre-training baseline", " In the `-pretrain' experiment, we remove the pre-training stage and directly update the model on three tasks, leading to a dramatic decrease on BLEU score, indicating the pre-training is an indispensable step for end-to-end ST Experiments ::: Discussion ::: Learning Curve It is interesting to investigate why our method is superior to baselines  We find that TCEN achieves a higher final result owing to a better start-point in fine-tuning"], "raptor_snippets": ["Introduction Speech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language BIBREF0; or in online courses, where audiences and speakers use different languages BIBREF1", " To tackle this problem, existing approaches can be categorized into cascaded method BIBREF2, BIBREF3, where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method BIBREF4, BIBREF5, where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest", " Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory Since it is hard to obtain a large-scale ST dataset, multi-task learning BIBREF5, BIBREF6 and pre-training techniques BIBREF7 have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT"]}
{"example_idx": 140, "doc_key": "2001.07820", "question": "What datasets do they use?", "overlap_recall": 0.043478260869565216, "baseline_ctx_tokens": 1997, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [35, 34, 46, 63, 16, 32, 33, 17, 68, 14, 43, 56, 38, 55, 44, 40, 57, 65, 62, 47, 21, 53, 37], "raptor_node_indices": [14, 1, 4, 16, 17, 11, 6, 9, 10, 7, 15, 5, 12, 3, 13, 8, 2, 0], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": [" These datasets present a variation in the text lengths (e g  the average number of words for yelp50, yelp200 and imdb400 is 34, 82 and 195 words respectively), training data size (e g  the number of training examples for target classifiers for imdb400, yelp50 and yelp200 are 18K, 407K and 2M, respectively) and input domain (e g  restaurant vs  movie reviews)", " We train and tune target classifiers (see Section SECREF8) using the training and development sets; and evaluate their performance on the original examples in the test sets as well as the adversarial examples generated by attacking methods for the test sets  Note that AutoEncoder also involves a training process, for which we train and tune AutoEncoder using the training and development sets in yelp50, yelp200 and imdb400  Statistics of the three datasets are presented in Table TABREF22", " We choose 3 ACC thresholds for the attacking performance: T0, T1 and T2, which correspond approximately to accuracy scores of 90%, 80% and 70% for the Yelp datasets (yelp50, yelp200); and 80%, 70% and 50% for the IMDB datasets (imdb400)  Each method is tuned accordingly to achieve a particular accuracy"], "raptor_snippets": ["Introduction Adversarial examples, a term introduced in BIBREF0, are inputs transformed by small perturbations that machine learning models consistently misclassify  The experiments are conducted in the context of computer vision (CV), and the core idea is encapsulated by an illustrative example: after imperceptible noises are added to a panda image, an image classifier predicts, with high confidence, that it is a gibbon", " Interestingly, these adversarial examples can also be used to improve the classifier — either as additional training data BIBREF0 or as a regularisation objective BIBREF1 — thus providing motivation for generating effective adversarial examples The germ of this paper comes from our investigation of adversarial attack methods for natural language processing (NLP) tasks, e g  sentiment classification, which drives us to quantify what is an “effective” or “good” adversarial example", " In the context of images, a good adversarial example is typically defined according two criteria: it has successfully fooled the classifier; it is visually similar to the original example In NLP, defining a good adversarial example is a little more involving, because while criterion (b) can be measured with a comparable text similarity metric (e g  BLEU or edit distance), an adversarial example should also: be fluent or natural; preserve its original label"]}
{"example_idx": 145, "doc_key": "1912.13337", "question": "After how many hops does accuracy decrease?", "overlap_recall": 0.043478260869565216, "baseline_ctx_tokens": 1914, "raptor_ctx_tokens": 1995, "leaf_token_frac": 0.899749373433584, "summary_token_frac": 0.10025062656641603, "baseline_indices": [93, 12, 101, 92, 11, 100, 103, 96, 75, 105, 74, 84, 106, 78, 89, 104, 10, 91, 98, 52, 113, 86, 7], "raptor_node_indices": [64, 35, 12, 41, 72, 68, 70, 47, 56, 39, 36, 73, 84, 54, 40, 37, 55, 51, 59, 63, 42, 87, 60, 65], "raptor_leaf_indices": [12, 16, 21, 35, 36, 37, 39, 40, 41, 42, 43, 47, 51, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 68, 70, 72, 73], "baseline_snippets": [" For example, problems that involve hyponym reasoning with sister distractors of distance $k^{\\prime }=1$ (i e , the second column) degrades from 47% to 15% when the number of hops $k$ increases from 1 to 4  This general tendency persists even after additional fine-tuning, as we discuss next, and gives evidence that models are limited in their capacity for certain types of multi-hop inferences", " For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops  Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions", " After 3k examples, the model has high performance on virtually all categories (the same score increases from 59% to 87%), however results still tends to degrade as a function of hop and distractor complexity, as discussed above Despite the high performance of our transformer models after inoculation, model performance on most probes (with the exception of Definitions) averages around 80% for our best models"], "raptor_snippets": [" For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops  Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions", "Our main study focuses on probing the BERT model and fine-tuning approach of BIBREF7, and other variants thereof, which are all based on the transformer architecture of BIBREF25  Related to our efforts, there have been recent studies into the types of relational knowledge contained in large-scale knowledge models BIBREF26, BIBREF27, BIBREF28, which, similar to our work, probe models using structured knowledge sources", " Each of our probing datasets consists of multiple-choice questions that include a question $\\textbf {q}$ and a set of answer choices or candidates $\\lbrace a_{1}, a_{N}\\rbrace $  This section describes in detail the 5 different datasets we build, which are drawn from two sources of expert knowledge, namely WordNet BIBREF35 and the GNU Collaborative International Dictionary of English (GCIDE)"]}
{"example_idx": 170, "doc_key": "1909.03582", "question": "What is this method improvement over the best performing state-of-the-art?", "overlap_recall": 0.043478260869565216, "baseline_ctx_tokens": 1914, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [55, 61, 37, 51, 46, 57, 52, 54, 56, 43, 58, 62, 38, 40, 66, 60, 30, 28, 59, 67, 17, 42, 39], "raptor_node_indices": [12, 19, 5, 18, 11, 15, 13, 4, 9, 20, 8, 10, 2, 1, 3, 14, 21, 6, 17, 7, 0, 16], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" By filtering out low-sensational headlines, Pointer-Gen+Same-FT and Pointer-Gen+Pos-FT achieves higher sensationalism scores, which implies the effectiveness of our sensationalism scorer  Our Pointer-Gen+ARL-SEN model achieves the best performance of 60 8%  This is an absolute improvement of 18 2% over the Pointer-Gen baseline", " And the color of orange/black further indicates the better model and its score  We find that Pointer-Gen+ARL-SEN outperforms Pointer-Gen+RL-SEN for most cases  The improvement is higher when the test set headlines are not sensational (the sensationalism score is less than 0 5), which may be attributed to the higher ratio of RL training on non-sensational headlines", "We experiment and compare with the following models Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13 Pointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0 5 Pointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0 1"], "raptor_snippets": ["Introduction Headline generation is the process of creating a headline-style sentence given an input article  The research community has been regarding the task of headline generation as a summarization task BIBREF1, ignoring the fundamental differences between headlines and summaries  While summaries aim to contain most of the important information from the articles, headlines do not necessarily need to  Instead, a good headline needs to capture people's attention and serve as an irresistible invitation for users to read through the article", " For example, the headline “$2 Billion Worth of Free Media for Trump”, which gives only an intriguing hint, is considered better than the summarization style headline “Measuring Trump’s Media Dominance” , as the former gets almost three times the readers as the latter  Generating headlines with many clicks is especially important in this digital age, because many of the revenues of journalism come from online advertisements and getting more user clicks means being more competitive in the market", " However, most existing websites naively generate sensational headlines using only keywords or templates  Instead, this paper aims to learn a model that generates sensational headlines based on an input article without labeled data To generate sensational headlines, there are two main challenges  Firstly, there is a lack of sensationalism scorer to measure how sensational a headline is  Some researchers have tried to manually label headlines as clickbait or non-clickbait BIBREF2, BIBREF3"]}
{"example_idx": 141, "doc_key": "2001.07820", "question": "What other factors affect the performance?", "overlap_recall": 0.045454545454545456, "baseline_ctx_tokens": 1908, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [49, 51, 67, 55, 40, 46, 41, 38, 34, 57, 52, 35, 45, 62, 6, 48, 56, 68, 37, 58, 32, 31], "raptor_node_indices": [11, 1, 12, 13, 16, 15, 5, 9, 7, 10, 14, 6, 4, 2, 8, 0, 17, 3], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": ["Comparing the performance across different ACC thresholds, we observe a consistent pattern of decreasing performance over all metrics as the attacking performance increases from T0 to T2  These observations suggest that all methods are trading off fluency and content preservation as they attempt to generate stronger adversarial examples We now focus on Table TABREF23A, to understand the impact of model architecture for the target classifier", " Looking at Table TABREF23B, we also find that the attacking performance is influenced by the input text length and the number of training examples for target classifiers  For HotFlip, we see improvements over BLEU and ACPT as text length increases from yelp50 to yelp200 to imdb400, indicating the performance of HotFlip is more affected by input lengths  We think this is because with more words it is more likely for HotFlip to find a vulnerable spot to target", "We propose an evaluation framework for assessing the quality of adversarial examples in NLP, based on four criteria: (a) attacking performance, (b) textual similarity; (c) fluency; (d) label preservation  Our framework involves both automatic and human evaluation, and we test 5 benchmark methods and a novel auto-encoder approach  We found that the architecture of the target classifier is an important factor when it comes to attacking performance, e g"], "raptor_snippets": ["Introduction Adversarial examples, a term introduced in BIBREF0, are inputs transformed by small perturbations that machine learning models consistently misclassify  The experiments are conducted in the context of computer vision (CV), and the core idea is encapsulated by an illustrative example: after imperceptible noises are added to a panda image, an image classifier predicts, with high confidence, that it is a gibbon", " Interestingly, these adversarial examples can also be used to improve the classifier — either as additional training data BIBREF0 or as a regularisation objective BIBREF1 — thus providing motivation for generating effective adversarial examples The germ of this paper comes from our investigation of adversarial attack methods for natural language processing (NLP) tasks, e g  sentiment classification, which drives us to quantify what is an “effective” or “good” adversarial example", " In the context of images, a good adversarial example is typically defined according two criteria: it has successfully fooled the classifier; it is visually similar to the original example In NLP, defining a good adversarial example is a little more involving, because while criterion (b) can be measured with a comparable text similarity metric (e g  BLEU or edit distance), an adversarial example should also: be fluent or natural; preserve its original label"]}
{"example_idx": 171, "doc_key": "1909.03582", "question": "Which baselines are used for evaluation?", "overlap_recall": 0.08695652173913043, "baseline_ctx_tokens": 1963, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [47, 57, 37, 51, 17, 55, 39, 60, 48, 56, 28, 14, 50, 62, 54, 38, 36, 52, 46, 49, 61, 43, 20], "raptor_node_indices": [19, 12, 9, 5, 15, 20, 10, 6, 8, 4, 2, 18, 17, 21, 3, 13, 16, 14, 1, 0, 11, 7], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": ["We stop training Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN, when $\\alpha _\\text{sen}$ stops increasing on the validation set  Beam-search with a beam size of 5 is adopted for decoding in all models Sensational Headline Generation ::: Evaluation Metrics We briefly describe the evaluation metrics below ROUGE: ROUGE is a commonly used evaluation metric for summarization", " On the other hand, we found that Pointer-Gen+Pos is much worse than other baselines  The reason is that training on sensational samples alone discards around 80% of the whole training set that is also helpful for maintaining relevance and a good language model  It shows the necessity of using RL UTF8gbsn", "We experiment and compare with the following models Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13 Pointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0 5 Pointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0 1"], "raptor_snippets": ["Introduction Headline generation is the process of creating a headline-style sentence given an input article  The research community has been regarding the task of headline generation as a summarization task BIBREF1, ignoring the fundamental differences between headlines and summaries  While summaries aim to contain most of the important information from the articles, headlines do not necessarily need to  Instead, a good headline needs to capture people's attention and serve as an irresistible invitation for users to read through the article", " For example, the headline “$2 Billion Worth of Free Media for Trump”, which gives only an intriguing hint, is considered better than the summarization style headline “Measuring Trump’s Media Dominance” , as the former gets almost three times the readers as the latter  Generating headlines with many clicks is especially important in this digital age, because many of the revenues of journalism come from online advertisements and getting more user clicks means being more competitive in the market", " However, most existing websites naively generate sensational headlines using only keywords or templates  Instead, this paper aims to learn a model that generates sensational headlines based on an input article without labeled data To generate sensational headlines, there are two main challenges  Firstly, there is a lack of sensationalism scorer to measure how sensational a headline is  Some researchers have tried to manually label headlines as clickbait or non-clickbait BIBREF2, BIBREF3"]}
{"example_idx": 114, "doc_key": "1911.02855", "question": "What are method improvements of F1 for paraphrase identification?", "overlap_recall": 0.09090909090909091, "baseline_ctx_tokens": 1912, "raptor_ctx_tokens": 1949, "leaf_token_frac": 0.7947665469471524, "summary_token_frac": 0.20523345305284763, "baseline_indices": [54, 62, 11, 35, 29, 36, 60, 33, 53, 2, 48, 50, 61, 55, 47, 46, 45, 51, 39, 56, 31, 41], "raptor_node_indices": [8, 21, 18, 14, 27, 3, 5, 23, 1, 2, 31, 33, 28, 25, 7, 34, 9, 6, 11, 0, 26, 32], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], "baseline_snippets": ["Paraphrases are textual expressions that have the same semantic meaning using different surface words  Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not  We use BERT BIBREF11 and XLNet BIBREF43 as backbones and report F1 score for comparison  Hyperparameters are tuned on the development set of each dataset Experiments ::: Paraphrase Identification ::: Datasets", " Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks", "29), OntoNotes5 0 (92 07, +0 96)), MSRA 96 72(+0 97) and OntoNotes4 0 (84 47,+2 36) for the NER task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification The rest of this paper is organized as follows: related work is presented in Section 2  We describe different training objectives in Section 3"], "raptor_snippets": ["Introduction Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension  Table TABREF3 gives concrete examples: for the Named Entity Recognition (NER) task BIBREF2, BIBREF3, most tokens are backgrounds with tagging class $O$  Specifically, the number of tokens tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5", "0 dataset; Data-imbalanced issue is more severe for MRC tasks BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 with the value of negative-positive ratio being 50-200 Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label", " This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function while at test time, F1 score concerns more about positive examples; (2) the overwhelming effect of easy-negative examples  As pointed out by meng2019dsreg, significantly large number of negative examples also means that the number of easy-negative example is large"]}
{"example_idx": 155, "doc_key": "1909.07575", "question": "What is the attention module pretrained on?", "overlap_recall": 0.125, "baseline_ctx_tokens": 1978, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [20, 5, 31, 39, 9, 23, 33, 32, 74, 22, 6, 86, 66, 79, 78, 68, 58, 44, 72, 67, 70, 69, 80, 90], "raptor_node_indices": [20, 15, 2, 13, 19, 18, 12, 6, 8, 21, 1, 9, 14, 3, 0, 5, 16, 17, 11, 7, 4, 10], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" It can be jointly trained on ASR, MT, and ST tasks  As the attention module is task-specific, three attentions are defined Usually, the size of $\\mathcal {A}$ and $\\mathcal {M}$ is much larger than $\\mathcal {S}$  Therefore, the common training practice is to pre-train the model on ASR and MT tasks and then fine-tune it with a multi-task learning manner", " The encoder is a pure acoustic model in pre-training, while it has to extract semantic and linguistic features additionally in fine-tuning, which significantly increases the learning difficulty Non-pre-trained Attention Module: Previous work BIBREF6 trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training", "Here, $z_k$ is the the hidden state of the deocder RNN at $k$ step and $c_k$ is a time-dependent context vector computed by the attention $att$ Our method ::: Training Procedure Following previous work, we split the training procedure to pre-training and fine-tuning stages"], "raptor_snippets": ["Introduction Speech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language BIBREF0; or in online courses, where audiences and speakers use different languages BIBREF1", " To tackle this problem, existing approaches can be categorized into cascaded method BIBREF2, BIBREF3, where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method BIBREF4, BIBREF5, where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest", " Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory Since it is hard to obtain a large-scale ST dataset, multi-task learning BIBREF5, BIBREF6 and pre-training techniques BIBREF7 have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT"]}
{"example_idx": 115, "doc_key": "1911.02855", "question": "What are method's improvements of F1 for NER task for English and Chinese datasets?", "overlap_recall": 0.13043478260869565, "baseline_ctx_tokens": 1985, "raptor_ctx_tokens": 1936, "leaf_token_frac": 0.7933884297520661, "summary_token_frac": 0.2066115702479339, "baseline_indices": [47, 60, 42, 46, 41, 43, 11, 10, 44, 51, 62, 2, 61, 36, 29, 53, 31, 55, 45, 0, 35, 52, 30], "raptor_node_indices": [14, 21, 23, 5, 19, 8, 33, 31, 27, 2, 18, 22, 28, 25, 1, 6, 7, 4, 34, 9, 20, 32], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], "baseline_snippets": [" We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0 97 and +2 36 on MSRA and OntoNotes4 0, respectively  As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets Experiments ::: Machine Reading Comprehension", "0 NER dataset and English QuoRef MRC dataset to examine the influence of tradeoff between precision and recall  Experiment results are shown in Table   The highest F1 for Chinese OntoNotes4 0 is 84 67 when $\\alpha $ is set to 0 6 while for QuoRef, the highest F1 is 68 44 when $\\alpha $ is set to 0 4", " Hyperparameters are tuned on the development set of each dataset Experiments ::: Named Entity Recognition ::: Datasets For the NER task, we consider both Chinese datasets, i e , OntoNotes4 0 BIBREF34 and MSRA BIBREF35, and English datasets, i e , CoNLL2003 BIBREF36 and OntoNotes5 0 BIBREF37"], "raptor_snippets": ["Introduction Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension  Table TABREF3 gives concrete examples: for the Named Entity Recognition (NER) task BIBREF2, BIBREF3, most tokens are backgrounds with tagging class $O$  Specifically, the number of tokens tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5", "0 dataset; Data-imbalanced issue is more severe for MRC tasks BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 with the value of negative-positive ratio being 50-200 Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label", " This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function while at test time, F1 score concerns more about positive examples; (2) the overwhelming effect of easy-negative examples  As pointed out by meng2019dsreg, significantly large number of negative examples also means that the number of easy-negative example is large"]}
{"example_idx": 116, "doc_key": "1911.02855", "question": "What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?", "overlap_recall": 0.13043478260869565, "baseline_ctx_tokens": 1981, "raptor_ctx_tokens": 1961, "leaf_token_frac": 0.8470168281489037, "summary_token_frac": 0.1529831718510964, "baseline_indices": [36, 39, 53, 45, 47, 46, 58, 10, 60, 35, 52, 44, 23, 43, 56, 62, 51, 37, 33, 16, 30, 42, 61], "raptor_node_indices": [5, 23, 21, 29, 31, 14, 30, 18, 2, 33, 27, 19, 17, 16, 8, 3, 11, 22, 28, 10, 15, 1, 13], "raptor_leaf_indices": [1, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 25, 26, 27, 28, 29], "baseline_snippets": [" In this paper, we choose BERT as the backbone and conduct experiments on three Chinese POS datasets  We report the span-level micro-averaged precision, recall and F1 for evaluation  Hyperparameters are tuned on the corresponding development set of each dataset Experiments ::: Part-of-Speech Tagging ::: Datasets We conduct experiments on the widely used Chinese Treebank 5 0, 6 0 as well as UD1 4", "Bert-Tagger: devlin2018bert treats part-of-speech as a tagging task Experiments ::: Part-of-Speech Tagging ::: Results Table presents the experimental results on the POS task  As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i e , outperforming BERT-tagger by +1 86 in terms of F1 score on CTB5, +1 80 on CTB6 and +2", " Moreover, on QuoRef, the proposed method surpasses XLNet results by +1 46 on EM and +1 41 on F1  Another observation is that, XLNet outperforms BERT by a huge margin, and the proposed DSC loss can obtain further performance improvement by an average score above 1 0 in terms of both EM and F1, which indicates the DSC loss is complementary to the model structures Experiments ::: Paraphrase Identification"], "raptor_snippets": ["0 dataset; Data-imbalanced issue is more severe for MRC tasks BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 with the value of negative-positive ratio being 50-200 Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label", " This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function while at test time, F1 score concerns more about positive examples; (2) the overwhelming effect of easy-negative examples  As pointed out by meng2019dsreg, significantly large number of negative examples also means that the number of easy-negative example is large", " The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learned to distinguish between positive examples and hard-negative examples"]}
{"example_idx": 169, "doc_key": "1909.03582", "question": "What is future work planed?", "overlap_recall": 0.16, "baseline_ctx_tokens": 1965, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [28, 10, 65, 23, 19, 39, 42, 5, 73, 36, 43, 24, 27, 29, 59, 67, 40, 25, 53, 47, 26, 20, 38, 14, 9], "raptor_node_indices": [20, 0, 15, 21, 19, 12, 17, 1, 5, 13, 14, 9, 3, 10, 16, 2, 8, 6, 18, 11, 7, 4], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": ["We use the baseline reward $\\hat{R_t}$ to reduce the variance of the reward, similar to ranzato2015sequence  To elaborate, a linear model is deployed to estimate the baseline reward $\\hat{R_t}$ based on $t$-th state $o_t$ for each timestep $t$  The parameters of the linear model are trained by minimizing the mean square loss between $R$ and $\\hat{R_t}$:", " China Railway opens new Shentong and Shunfeng special lines) will become UTF8gbsn“中铁总将增开京广两列快递专列\" (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of UTF8gbsn“一趟挣10万？\" (One trip to earn 100 thousand ) ", " As long summaries were recognized as important, the CNN/Daily Mail dataset was used in nallapati2016abstractive  Graph-based attention BIBREF16, pointer-generator with coverage loss BIBREF0 are further developed to improve the generated summaries  celikyilmaz2018deep proposed deep communicating agents for representing a long document for abstractive summarization"], "raptor_snippets": ["Introduction Headline generation is the process of creating a headline-style sentence given an input article  The research community has been regarding the task of headline generation as a summarization task BIBREF1, ignoring the fundamental differences between headlines and summaries  While summaries aim to contain most of the important information from the articles, headlines do not necessarily need to  Instead, a good headline needs to capture people's attention and serve as an irresistible invitation for users to read through the article", " For example, the headline “$2 Billion Worth of Free Media for Trump”, which gives only an intriguing hint, is considered better than the summarization style headline “Measuring Trump’s Media Dominance” , as the former gets almost three times the readers as the latter  Generating headlines with many clicks is especially important in this digital age, because many of the revenues of journalism come from online advertisements and getting more user clicks means being more competitive in the market", " However, most existing websites naively generate sensational headlines using only keywords or templates  Instead, this paper aims to learn a model that generates sensational headlines based on an input article without labeled data To generate sensational headlines, there are two main challenges  Firstly, there is a lack of sensationalism scorer to measure how sensational a headline is  Some researchers have tried to manually label headlines as clickbait or non-clickbait BIBREF2, BIBREF3"]}
{"example_idx": 133, "doc_key": "1805.04508", "question": "Which race and gender are given higher sentiment intensity predictions?", "overlap_recall": 0.20833333333333334, "baseline_ctx_tokens": 1995, "raptor_ctx_tokens": 1942, "leaf_token_frac": 0.8455200823892894, "summary_token_frac": 0.15447991761071062, "baseline_indices": [66, 63, 55, 18, 42, 57, 43, 58, 50, 46, 10, 44, 9, 38, 4, 70, 45, 26, 69, 59, 8, 35, 11, 7], "raptor_node_indices": [28, 23, 29, 2, 21, 20, 25, 22, 26, 5, 33, 12, 7, 27, 24, 30, 13, 1, 17, 16, 34, 0, 10], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": [" We found that more than 75% of the systems tend to mark sentences involving one gender/race with higher intensity scores than the sentences involving the other gender/race  We found such biases to be more widely prevalent for race than for gender  We also found that the bias can be different depending on the particular affect dimension involved", "002 on the race sentence pairs  The predicted intensity scores tended to be higher on the sentences with male noun phrases than on the sentences with female noun phrases for the tasks of anger, fear, and sadness intensity prediction  This tendency was reversed on the task of valence prediction  On the race sentence pairs, the system predicted higher intensity scores on the sentences with European American names for all four emotion intensity prediction tasks, and on the sentences with African American names for the task of valence prediction", " This indicates that gender-associated words can have a bigger impact on system predictions for neutral sentences We also performed an analysis by restricting the dataset to contain only the sentences with the emotion words corresponding to the emotion task (i e , submissions to the anger intensity prediction task were evaluated only on sentences with anger words)  The results (not shown here) were similar to the results on the full set Race Bias Results We did a similar analysis for race as we did for gender"], "raptor_snippets": ["Introduction [0]leftmargin=* [0]leftmargin=* Automatic systems have had a significant and beneficial impact on all walks of human life  So much so that it is easy to overlook their potential to benefit society by promoting equity, diversity, and fairness", " For example, machines do not take bribes to do their jobs, they can determine eligibility for a loan without being influenced by the color of the applicant's skin, and they can provide access to information and services without discrimination based on gender or sexual orientation  Nonetheless, as machine learning systems become more human-like in their predictions, they can also perpetuate human biases  Some learned biases may be beneficial for the downstream application (e g", ", learning that humans often use some insect names, such as spider or cockroach, to refer to unpleasant situations)  Other biases can be inappropriate and result in negative experiences for some groups of people"]}
{"example_idx": 65, "doc_key": "1911.10049", "question": "What is the improvement in performance for Estonian in the NER task?", "overlap_recall": 0.21739130434782608, "baseline_ctx_tokens": 1952, "raptor_ctx_tokens": 1938, "leaf_token_frac": 0.7936016511867905, "summary_token_frac": 0.20639834881320948, "baseline_indices": [41, 47, 26, 43, 37, 39, 40, 45, 42, 19, 49, 35, 14, 38, 34, 36, 48, 10, 30, 46, 29, 24, 2], "raptor_node_indices": [23, 21, 19, 14, 4, 28, 22, 27, 8, 24, 32, 31, 5, 18, 34, 25, 12, 20, 6, 33, 1, 13], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], "baseline_snippets": [" NER is an information extraction task that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc  To allow comparison of results between languages, we used an adapted version of this task, which uses a reduced set of labels, available in NER datasets for all processed languages", " By training and evaluating multiple times, we minimise this random component The results are presented in Table TABREF21  We included the evaluation of the original ELMo English model in the same table  NER models have little difficulty distinguishing between types of named entities, but recognizing whether a word is a named entity or not is more difficult  For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings", "We evaluated the produced ELMo models for all languages using two evaluation tasks: a word analogy task and named entity recognition (NER) task  Below, we first shortly describe each task, followed by the evaluation results Evaluation ::: Word Analogy Task The word analogy task was popularized by mikolov2013distributed"], "raptor_snippets": ["Introduction Word embeddings are representations of words in numerical form, as vectors of typically several hundred dimensions  The vectors are used as an input to machine learning models; for complex language processing tasks these are typically deep neural networks  The embedding vectors are obtained from specialized learning tasks, based on neural networks, e g , word2vec BIBREF0, GloVe BIBREF1, FastText BIBREF2, ELMo BIBREF3, and BERT BIBREF4", " For training, the embeddings algorithms use large monolingual corpora that encode important information about word meaning as distances between vectors  In order to enable downstream machine learning on text understanding tasks, the embeddings shall preserve semantic relations between words, and this is true even across languages Probably the best known word embeddings are produced by the word2vec method BIBREF5  The problem with word2vec embeddings is their failure to express polysemous words", " During training of an embedding, all senses of a given word (e g , paper as a material, as a newspaper, as a scientific work, and as an exam) contribute relevant information in proportion to their frequency in the training corpus  This causes the final vector to be placed somewhere in the weighted middle of all words' meanings  Consequently, rare meanings of words are poorly expressed with word2vec and the resulting vectors do not offer good semantic representations"]}
{"example_idx": 147, "doc_key": "1912.13337", "question": "Is WordNet useful for taxonomic reasoning for this task?", "overlap_recall": 0.24, "baseline_ctx_tokens": 1951, "raptor_ctx_tokens": 1924, "leaf_token_frac": 0.7920997920997921, "summary_token_frac": 0.2079002079002079, "baseline_indices": [33, 8, 32, 31, 43, 30, 102, 24, 23, 18, 47, 48, 35, 46, 40, 21, 110, 39, 17, 36, 19, 44, 107, 104, 79], "raptor_node_indices": [47, 26, 38, 33, 57, 6, 69, 84, 34, 86, 76, 75, 78, 42, 72, 79, 68, 59, 24, 29, 13, 25, 22], "raptor_leaf_indices": [6, 11, 13, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 41, 42, 47, 54, 55, 57, 59, 60, 61, 64, 65, 67, 68, 69, 71, 72, 75, 76], "baseline_snippets": ["We build 4 individual datasets based on semantic relations native to WordNet (see BIBREF37): hypernymy (i e , generalization or ISA reasoning up a taxonomy, ISA$^\\uparrow $), hyponymy (ISA$^{\\downarrow }$), synonymy, and definitions  To generate a set of questions in each case, we employ a number of rule templates $\\mathcal {Q}$ that operate over tuples", " For sources of expert knowledge, we use WordNet, a comprehensive lexical ontology, and other publicly available dictionary resources  We devise probes that measure model competence in definition and taxonomic knowledge in different settings (including hypernymy, hyponymy, and synonymy detection, and word sense disambiguation)", " Our main motivation for using WordNet, as opposed to a resource such as ConceptNet BIBREF36, is the availability of glosses ($\\mathcal {D}$) and example sentences ($\\mathcal {S}$), which allows us to construct natural language questions that contextualize the types of concepts we want to probe Dataset Probes and Construction ::: WordNetQA ::: Example Generation @ START@$\\textsc {gen}(\\tau )$@ END@"], "raptor_snippets": [" Our general methodology works as illustrated in Figure FIGREF1: given any MCQA model trained on a set of benchmark tasks, we systematically generate a set of synthetic dataset probes (i e , MCQA renderings of the target information) from information in expert knowledge sources", " We also show that the same models can be effectively re-fine-tuned on small samples (even 100 examples) of probe data, and that high performance on the probes tends to correlate with a smaller drop in the model's performance on the original QA task Our comprehensive assessment reveals several interesting nuances to the overall positive trend", " State-of-the-art QA models thus have much room to improve even in some fundamental building blocks, namely definitions and taxonomic hierarchies, of more complex forms of reasoning Related Work We follow recent work on constructing challenge datasets for probing neural models, which has primarily focused on the task of natural language inference (NLI) BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18"]}
{"example_idx": 172, "doc_key": "1909.03582", "question": "Did they used dataset from another domain for evaluation?", "overlap_recall": 0.25, "baseline_ctx_tokens": 1965, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [36, 51, 15, 16, 73, 14, 60, 69, 62, 48, 17, 56, 50, 64, 37, 66, 10, 57, 49, 65, 8, 61, 47, 55], "raptor_node_indices": [18, 9, 5, 8, 15, 12, 6, 19, 10, 13, 16, 21, 11, 1, 2, 20, 3, 0, 14, 7, 4, 17], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" The dataset is collected from the Chinese microblogging website Sina Weibo  It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text  The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing  We tokenize each sentence with Jieba and a vocabulary size of 50000 is saved Sensational Headline Generation ::: Baselines and Our Models", "We first compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation  Note that we only compare our models to commonly used strong summarization baselines, to validate that our implementation achieves comparable performance to existing work", " To evaluate our trained classifier, we construct a test set by randomly sampling 100 headlines from the test split of LCSTS dataset and the labels are obtained by 11 human annotators  Annotations show that 52% headlines are labeled as positive and 48% headlines as negative by majority voting (The detail on the annotation can be found in Section SECREF26) Sensationalism Scorer ::: Results and Discussion Our classifier achieves 0 65 accuracy and 0"], "raptor_snippets": ["Introduction Headline generation is the process of creating a headline-style sentence given an input article  The research community has been regarding the task of headline generation as a summarization task BIBREF1, ignoring the fundamental differences between headlines and summaries  While summaries aim to contain most of the important information from the articles, headlines do not necessarily need to  Instead, a good headline needs to capture people's attention and serve as an irresistible invitation for users to read through the article", " For example, the headline “$2 Billion Worth of Free Media for Trump”, which gives only an intriguing hint, is considered better than the summarization style headline “Measuring Trump’s Media Dominance” , as the former gets almost three times the readers as the latter  Generating headlines with many clicks is especially important in this digital age, because many of the revenues of journalism come from online advertisements and getting more user clicks means being more competitive in the market", " However, most existing websites naively generate sensational headlines using only keywords or templates  Instead, this paper aims to learn a model that generates sensational headlines based on an input article without labeled data To generate sensational headlines, there are two main challenges  Firstly, there is a lack of sensationalism scorer to measure how sensational a headline is  Some researchers have tried to manually label headlines as clickbait or non-clickbait BIBREF2, BIBREF3"]}
{"example_idx": 122, "doc_key": "1911.00841", "question": "Were other baselines tested to compare with the neural baseline?", "overlap_recall": 0.2608695652173913, "baseline_ctx_tokens": 1907, "raptor_ctx_tokens": 1986, "leaf_token_frac": 0.798590130916415, "summary_token_frac": 0.2014098690835851, "baseline_indices": [36, 40, 30, 55, 37, 41, 9, 42, 51, 34, 20, 35, 39, 57, 60, 44, 53, 29, 31, 25, 28, 18, 43], "raptor_node_indices": [31, 22, 29, 5, 23, 21, 19, 27, 25, 18, 26, 32, 28, 33, 17, 13, 3, 14, 24, 1, 34, 8, 2], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": ["No-Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally-sound way on the basis of information present in the privacy policy  We establish a simple baseline to quantify the effect of identifying every question as unanswerable", "The results of the answerability baselines are presented in Table TABREF31, and on answer sentence selection in Table TABREF32  We observe that bert exhibits the best performance on a binary answerability identification task  However, most baselines considerably exceed the performance of a majority-class baseline  This suggests considerable information in the question, indicating it's possible answerability within this domain Table TABREF32 describes the performance of our baselines on the answer sentence selection task", "We evaluate the ability of machine learning methods to identify relevant evidence for questions in the privacy domain  We establish baselines for the subtask of deciding on the answerability (§SECREF33) of a question, as well as the overall task of identifying evidence for questions from policies (§SECREF37)  We describe aspects of the question that can render it unanswerable within the privacy domain (§SECREF41) Experimental Setup ::: Answerability Identification Baselines"], "raptor_snippets": ["Introduction Privacy policies are the documents which disclose the ways in which a company gathers, uses, shares and manages a user's data  As legal documents, they function using the principle of notice and choice BIBREF0, where companies post their policies, and theoretically, users read the policies and decide to use a company's products or services only if they find the conditions outlined in its privacy policy acceptable", " Many legal jurisdictions around the world accept this framework, including the United States and the European Union BIBREF1, BIBREF2  However, the legitimacy of this framework depends upon users actually reading and understanding privacy policies to determine whether company practices are acceptable to them BIBREF3  In practice this is seldom the case BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10", " This is further complicated by the highly individual and nuanced compromises that users are willing to make with their data BIBREF11, discouraging a `one-size-fits-all' approach to notice of data practices in privacy documents With devices constantly monitoring our environment, including our personal space and our bodies, lack of awareness of how our data is being used easily leads to problematic situations where users are outraged by information misuse, but companies insist that users have consented"]}
{"example_idx": 121, "doc_key": "1911.00841", "question": "What type of neural model was used?", "overlap_recall": 0.2916666666666667, "baseline_ctx_tokens": 1975, "raptor_ctx_tokens": 1936, "leaf_token_frac": 0.7417355371900827, "summary_token_frac": 0.25826446280991733, "baseline_indices": [33, 55, 57, 39, 34, 38, 36, 30, 11, 59, 31, 3, 60, 42, 37, 16, 7, 41, 19, 9, 40, 43, 35, 18], "raptor_node_indices": [31, 9, 29, 5, 27, 33, 28, 6, 34, 25, 7, 11, 26, 18, 24, 2, 23, 3, 22, 8, 30, 32], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": [" This results in vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel CNN: We utilize a CNN neural encoder for answerability prediction  We use GloVe word embeddings BIBREF50, and a filter size of 5 with 64 filters to encode questions BERT: BERT BIBREF51 is a bidirectional transformer-based language-model BIBREF52", " The goal of this work is to promote question-answering research in the specialized privacy domain, where it can have large real-world impact  Strong neural baselines on PrivacyQA achieve a performance of only 39 8 F1 on this corpus, indicating considerable room for future research  Further, we shed light on several important considerations that affect the answerability of questions", "This research was supported in part by grants from the National Science Foundation Secure and Trustworthy Computing program (CNS-1330596, CNS-1330214, CNS-15-13957, CNS-1801316, CNS-1914486, CNS-1914444) and a DARPA Brandeis grant on Personalized Privacy Assistants (FA8750-15-2-0277)"], "raptor_snippets": ["Introduction Privacy policies are the documents which disclose the ways in which a company gathers, uses, shares and manages a user's data  As legal documents, they function using the principle of notice and choice BIBREF0, where companies post their policies, and theoretically, users read the policies and decide to use a company's products or services only if they find the conditions outlined in its privacy policy acceptable", " Many legal jurisdictions around the world accept this framework, including the United States and the European Union BIBREF1, BIBREF2  However, the legitimacy of this framework depends upon users actually reading and understanding privacy policies to determine whether company practices are acceptable to them BIBREF3  In practice this is seldom the case BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10", " This is further complicated by the highly individual and nuanced compromises that users are willing to make with their data BIBREF11, discouraging a `one-size-fits-all' approach to notice of data practices in privacy documents With devices constantly monitoring our environment, including our personal space and our bodies, lack of awareness of how our data is being used easily leads to problematic situations where users are outraged by information misuse, but companies insist that users have consented"]}
{"example_idx": 143, "doc_key": "1912.13337", "question": "Are the automatically constructed datasets subject to quality control?", "overlap_recall": 0.2916666666666667, "baseline_ctx_tokens": 1958, "raptor_ctx_tokens": 1983, "leaf_token_frac": 0.7982854261220373, "summary_token_frac": 0.20171457387796268, "baseline_indices": [112, 50, 3, 20, 115, 94, 114, 49, 13, 86, 83, 59, 6, 113, 76, 18, 21, 66, 85, 107, 46, 14, 51, 10], "raptor_node_indices": [68, 84, 63, 60, 41, 42, 11, 62, 52, 75, 55, 71, 56, 64, 38, 9, 21, 59, 72, 37, 87, 79, 85, 34], "raptor_leaf_indices": [9, 10, 11, 12, 16, 21, 34, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 71, 72, 73, 75], "baseline_snippets": ["The main appeal of using automatically generate datasets is the ability to systematically manipulate and control the complexity of target questions, which allows for more controlled experimentation and new forms of evaluation  Despite the positive results described above, results that look directly at the effect of different types of distractors and the complexity of reasoning show that our best models, even after additional fine-tuning, struggle with certain categories of hard distractors and multi-hop inferences", " After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e g , the first two entries in Table TABREF14) had a surprising correlation with such biases  This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5) Probing Methodology and Modeling", "Most existing MCQA datasets are constructed through either expensive crowd-sourcing BIBREF8 or hand engineering effort, in the former case making it possible to collect large amounts of data at the cost of losing systematic control over the semantics of the target questions  Hence, doing a controlled experiment to answer such a question for QA is difficult given a lack of targeted challenge datasets Having definitive empirical evidence of model competence on any given phenomenon requires constructing a wide range of systematic tests"], "raptor_snippets": [" This choice is motivated by fact that the science domain is considered particularly challenging for QA BIBREF10, BIBREF11, BIBREF12, and existing science benchmarks are known to involve widespread use of such knowledge (see BIBREF1, BIBREF13 for analysis), which is also arguably fundamental to more complex forms of reasoning We show that accurately probing QA models via synthetic datasets is not straightforward, as unexpected artifacts can easily arise in such data", " This motivates our carefully constructed baselines and close data inspection to ensure probe quality Our results confirm that transformer-based QA models have a remarkable ability to recognize certain types of knowledge captured in our probes—even without additional fine-tuning  Such models can even outperform strong task-specific models trained directly on our probing tasks (e g , on definitions, our best model achieves 77% test accuracy without specialized training, as opposed to 51% for a task-specific LSTM-based model)", " We also show that the same models can be effectively re-fine-tuned on small samples (even 100 examples) of probe data, and that high performance on the probes tends to correlate with a smaller drop in the model's performance on the original QA task Our comprehensive assessment reveals several interesting nuances to the overall positive trend"]}
{"example_idx": 166, "doc_key": "2001.02284", "question": "Do they use off-the-shelf NLP systems to build their assitant?", "overlap_recall": 0.2916666666666667, "baseline_ctx_tokens": 1955, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [74, 77, 4, 5, 52, 13, 17, 76, 3, 80, 6, 2, 49, 73, 37, 81, 25, 61, 18, 56, 78, 40, 7, 69], "raptor_node_indices": [8, 14, 4, 11, 1, 2, 16, 12, 15, 10, 3, 17, 13, 6, 7, 5, 9, 0], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": ["Rule-based Approaches: Though many of the latest research approaches handle NLU and NLG units by using statistical NLP models BIBREF5, BIBREF6, BIBREF7, most of the industrially deployed dialogue systems still use manual features or handcrafted rules for the state and action prediction, intent classification, and slot filling tasks BIBREF8, BIBREF9", " Despite having better adaptability compared to any rule-based system and being easy to train, end-to-end approaches remain unattainable for commercial conversational agents operating on real-world data  A well and carefully constructed task-oriented dialogue system in a known domain using handcrafted rules and predefined responses, still outperforms the end-to-end systems due to its robustness BIBREF11, BIBREF12", ", answering of FAQs), allowing human workers to focus on more sophisticated inquiries Recent research in the dialogue generation domain is conducted by employing AI-techniques like machine and deep learning BIBREF1, BIBREF2  However, conventional supervised methods have limitations when applied to real-world data and industrial tasks  The primary challenge here refers to a training phase since a robust model requires an extensive amount of structured and labeled data, that is often not available for domain-specific problems"], "raptor_snippets": ["Introduction Robotic Process Automation (RPA) is a type of software bots that simulates hand-operated human activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows BIBREF0  However, one of the drawbacks of RPA-bots is their susceptibility to changes in defined scenarios: being designed for a particular task, the RPA-bot is usually not adaptable to other domains or even light modifications in a workflow BIBREF0", " This inability to readjust to shifting conditions gave rise to Intelligent Process Automation (IPA) systems  IPA-bots combine RPA with Artificial Intelligence (AI) and thus are able to execute more cognitively demanding tasks that require i a  reasoning and language understanding  Hence, IPA-bots advanced beyond automating shallow “click tasks” and can perform jobs more intelligently – by means of machine learning algorithms", " Such IPA-systems undertake time-consuming and routine tasks, and thus enable smart workflows and free up skilled workers to accomplish higher-value activities One of the potential applications of Natural Language Processing (NLP) within the IPA domain are conversational interfaces that enable human-to-machine interaction  The main benefit of conversational systems is their ability to give attention to several users simultaneously while supporting natural communication"]}
{"example_idx": 134, "doc_key": "1805.04508", "question": "What criteria are used to select the 8,640 English sentences?", "overlap_recall": 0.30434782608695654, "baseline_ctx_tokens": 1960, "raptor_ctx_tokens": 1979, "leaf_token_frac": 0.797877716018191, "summary_token_frac": 0.20212228398180898, "baseline_indices": [25, 7, 18, 32, 19, 65, 53, 24, 35, 42, 31, 39, 43, 56, 9, 44, 45, 57, 20, 49, 68, 52, 14], "raptor_node_indices": [1, 34, 25, 28, 2, 32, 7, 0, 20, 33, 12, 26, 5, 23, 18, 24, 4, 29, 30, 13, 17, 14, 8], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": [" We manually examined the sentences to make sure they were grammatically well-formed  Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e g , `My daughter feels devastated' and `My son feels devastated')  We refer to the full set of 8,640 sentences as Equity Evaluation Corpus Measuring Race and Gender Bias in Automatic Sentiment Analysis Systems", " In this paper, we describe how we compiled a dataset of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders  We will refer to it as the Equity Evaluation Corpus (EEC)  We used the EEC as a supplementary test set in a recent shared task on predicting sentiment and emotion intensity in tweets, SemEval-2018 Task 1: Affect in Tweets BIBREF5 ", "We now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or lower) sentiment intensity scores to sentences involving a particular race or gender  There are several ways in which such a dataset may be compiled  We present below the choices that we made We decided to use sentences involving at least one race- or gender-associated word  The sentences were intended to be short and grammatically simple"], "raptor_snippets": ["Introduction [0]leftmargin=* [0]leftmargin=* Automatic systems have had a significant and beneficial impact on all walks of human life  So much so that it is easy to overlook their potential to benefit society by promoting equity, diversity, and fairness", " For example, machines do not take bribes to do their jobs, they can determine eligibility for a loan without being influenced by the color of the applicant's skin, and they can provide access to information and services without discrimination based on gender or sexual orientation  Nonetheless, as machine learning systems become more human-like in their predictions, they can also perpetuate human biases  Some learned biases may be beneficial for the downstream application (e g", ", learning that humans often use some insect names, such as spider or cockroach, to refer to unpleasant situations)  Other biases can be inappropriate and result in negative experiences for some groups of people"]}
{"example_idx": 142, "doc_key": "2001.07820", "question": "What are the benchmark attacking methods?", "overlap_recall": 0.30434782608695654, "baseline_ctx_tokens": 1953, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [46, 21, 17, 10, 7, 6, 67, 53, 55, 49, 34, 40, 45, 65, 9, 36, 68, 11, 51, 5, 1, 54, 32], "raptor_node_indices": [11, 12, 13, 15, 14, 6, 9, 8, 7, 2, 1, 16, 17, 3, 10, 4, 5, 0], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": [" We choose 3 ACC thresholds for the attacking performance: T0, T1 and T2, which correspond approximately to accuracy scores of 90%, 80% and 70% for the Yelp datasets (yelp50, yelp200); and 80%, 70% and 50% for the IMDB datasets (imdb400)  Each method is tuned accordingly to achieve a particular accuracy", "The benchmark methods we test (Section SECREF9) are gradient-based and optimisation-based attacks (Section SECREF2)  We propose an alternative model-based method to train a seperate generative model to generate text adversarial examples", " That is, we use the adversarial examples generated for attacking the three previous classifiers (BiLSTM, BiLSTM$+$A and CNN) as test data for BERT to measure its classification performance to understand whether these adversarial examples can fool BERT Methodology ::: Benchmark Attacking Methods We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4"], "raptor_snippets": ["Introduction Adversarial examples, a term introduced in BIBREF0, are inputs transformed by small perturbations that machine learning models consistently misclassify  The experiments are conducted in the context of computer vision (CV), and the core idea is encapsulated by an illustrative example: after imperceptible noises are added to a panda image, an image classifier predicts, with high confidence, that it is a gibbon", " Interestingly, these adversarial examples can also be used to improve the classifier — either as additional training data BIBREF0 or as a regularisation objective BIBREF1 — thus providing motivation for generating effective adversarial examples The germ of this paper comes from our investigation of adversarial attack methods for natural language processing (NLP) tasks, e g  sentiment classification, which drives us to quantify what is an “effective” or “good” adversarial example", " In the context of images, a good adversarial example is typically defined according two criteria: it has successfully fooled the classifier; it is visually similar to the original example In NLP, defining a good adversarial example is a little more involving, because while criterion (b) can be measured with a comparable text similarity metric (e g  BLEU or edit distance), an adversarial example should also: be fluent or natural; preserve its original label"]}
{"example_idx": 125, "doc_key": "1910.12574", "question": "What biases does their model capture?", "overlap_recall": 0.3076923076923077, "baseline_ctx_tokens": 1979, "raptor_ctx_tokens": 1971, "leaf_token_frac": 0.7970573313039067, "summary_token_frac": 0.20294266869609334, "baseline_indices": [68, 67, 20, 10, 55, 64, 65, 54, 51, 29, 26, 49, 59, 21, 27, 7, 50, 63, 13, 17, 5, 56, 52, 57, 33, 9], "raptor_node_indices": [36, 48, 35, 53, 51, 49, 15, 19, 45, 46, 11, 16, 44, 29, 13, 26, 24, 27, 25, 8, 40, 28, 7, 43], "raptor_leaf_indices": [7, 8, 11, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 40, 42, 43, 44, 45, 46], "baseline_snippets": [" It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data", " The evaluation results indicate that our model outperforms previous works by profiting the syntactical and contextual information embedded in different transformer encoder layers of the BERT model using a CNN-based fine-tuning strategy  Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets", " Furthermore, researchers have recently focused on the bias derived from the hate speech training datasets BIBREF18, BIBREF2, BIBREF19  Davidson et al  BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection  Wiegand et al"], "raptor_snippets": [" These newer models are applying deep learning approaches such as Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc BIBREF6, BIBREF0 to enhance the performance of hate speech detection models, however, they still suffer from lack of labelled data or inability to improve generalization property", "Here, we propose a transfer learning approach for hate speech understanding using a combination of the unsupervised pre-trained model BERT BIBREF11 and some new supervised fine-tuning strategies  As far as we know, it is the first time that such exhaustive fine-tuning strategies are proposed along with a generative pre-trained language model to transfer learning to low-resource hate speech languages and improve performance of the task  In summary:", "Previous Works Here, the existing body of knowledge on online hate speech and offensive language and transfer learning is presented Online Hate Speech and Offensive Language: Researchers have been studying hate speech on social media platforms such as Twitter BIBREF9, Reddit BIBREF12, BIBREF13, and YouTube BIBREF14 in the past few years"]}
{"example_idx": 135, "doc_key": "1805.06966", "question": "by how much did nus outperform abus?", "overlap_recall": 0.3181818181818182, "baseline_ctx_tokens": 1933, "raptor_ctx_tokens": 1973, "leaf_token_frac": 0.8479472883933097, "summary_token_frac": 0.15205271160669032, "baseline_indices": [62, 67, 68, 63, 65, 58, 64, 69, 5, 59, 66, 4, 55, 49, 23, 19, 48, 56, 6, 2, 24, 61], "raptor_node_indices": [23, 29, 21, 20, 22, 30, 28, 7, 24, 32, 17, 13, 2, 14, 15, 25, 33, 27, 16, 1, 19, 0, 18], "raptor_leaf_indices": [0, 1, 2, 4, 5, 6, 7, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": ["0% and of 96 6% when tested on the ABUS and the NUS, respectively  By comparison, the policies trained with the ABUS achieved average SRs of 99 5% and 45 5% respectively  Thus, training with the NUS leads to policies that can perform well on both USs, which is not the case for training with the ABUS  Furthermore, the best SRs when tested on the ABUS are similar at 99", " The best performing policy trained on the NUS achieves a 93 4% success rate and 13 8 average rewards whilst the best performing policy trained with the ABUS achieves only a 90 0% success rate and 13 3 average reward  This shows that the good performance of the NUS on the cross-model evaluation transfers to real users  Furthermore, the overfitting to a particular US is also observed in the real user evaluation", " For not only the policies trained on the NUS, but also those trained on the ABUS, the best performing policy was the policy that performed best on the other US Conclusion We introduced the Neural User Simulator (NUS), which uses the system's response in its semantic form as input and gives a natural language response  It thus needs less labelling of the training data than User Simulators that generate a response in semantic form"], "raptor_snippets": ["Introduction Spoken Dialogue Systems (SDS) allow human-computer interaction using natural speech  Task-oriented dialogue systems, the focus of this work, help users achieve goals such as finding restaurants or booking flights BIBREF0  Teaching a system how to respond appropriately in a task-oriented setting is non-trivial", " In state-of-the-art systems this dialogue management task is often formulated as a reinforcement learning (RL) problem BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3   In this framework, the system learns by a trial and error process governed by a reward function  User Simulators can be used to train the policy of a dialogue manager (DM) without real user interactions", " Furthermore, they allow an unlimited number of dialogues to be created with each dialogue being faster than a dialogue with a human In this paper the Neural User Simulator (NUS) is introduced which outputs natural language and whose behaviour is learned from a corpus  The main component, inspired by BIBREF4 , consists of a feature extractor and a neural network based sequence-to-sequence model BIBREF5 "]}
{"example_idx": 167, "doc_key": "2001.02284", "question": "How does the IPA label data after interacting with users?", "overlap_recall": 0.3181818181818182, "baseline_ctx_tokens": 1931, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [2, 12, 6, 34, 7, 1, 60, 59, 38, 13, 51, 25, 58, 32, 56, 37, 53, 33, 50, 40, 8, 46], "raptor_node_indices": [10, 4, 14, 6, 9, 8, 7, 12, 11, 2, 5, 17, 1, 15, 13, 16, 3, 0], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": [" Such IPA-systems undertake time-consuming and routine tasks, and thus enable smart workflows and free up skilled workers to accomplish higher-value activities One of the potential applications of Natural Language Processing (NLP) within the IPA domain are conversational interfaces that enable human-to-machine interaction  The main benefit of conversational systems is their ability to give attention to several users simultaneously while supporting natural communication", " This means that a tutor has to request the same information every time a new dialogue opens, which is very time consuming and could be successfully solved by means of an IPA dialogue bot Model The main objective of the proposed system is to interact with students at the beginning of every conversation and gather information on the topic (and sub-topic), examination mode and level, question number and exact problem formulation  Therefore, the system saves time for tutors and allows them to handle solely complex mathematical questions", " Such algorithms enable training of the systems with less or even a minimal amount of data, and are able to transfer the knowledge obtained during the training on existing data to the unseen domain Introduction ::: Outline and Contributions This paper addresses the challenge of implementing a dialogue system for IPA purposes within the practical e-learning domain with the initial absence of training data  Our contributions within this work are as follows:"], "raptor_snippets": ["Introduction Robotic Process Automation (RPA) is a type of software bots that simulates hand-operated human activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows BIBREF0  However, one of the drawbacks of RPA-bots is their susceptibility to changes in defined scenarios: being designed for a particular task, the RPA-bot is usually not adaptable to other domains or even light modifications in a workflow BIBREF0", " This inability to readjust to shifting conditions gave rise to Intelligent Process Automation (IPA) systems  IPA-bots combine RPA with Artificial Intelligence (AI) and thus are able to execute more cognitively demanding tasks that require i a  reasoning and language understanding  Hence, IPA-bots advanced beyond automating shallow “click tasks” and can perform jobs more intelligently – by means of machine learning algorithms", " Such IPA-systems undertake time-consuming and routine tasks, and thus enable smart workflows and free up skilled workers to accomplish higher-value activities One of the potential applications of Natural Language Processing (NLP) within the IPA domain are conversational interfaces that enable human-to-machine interaction  The main benefit of conversational systems is their ability to give attention to several users simultaneously while supporting natural communication"]}
{"example_idx": 144, "doc_key": "1912.13337", "question": "Do they focus on Reading Comprehension or multiple choice question answering?", "overlap_recall": 0.34782608695652173, "baseline_ctx_tokens": 1949, "raptor_ctx_tokens": 1987, "leaf_token_frac": 0.7986914947156517, "summary_token_frac": 0.20130850528434827, "baseline_indices": [2, 21, 76, 67, 0, 54, 83, 45, 17, 90, 1, 82, 15, 78, 48, 55, 18, 4, 20, 100, 108, 107, 3], "raptor_node_indices": [84, 37, 85, 60, 75, 72, 4, 64, 22, 80, 47, 33, 54, 10, 6, 13, 76, 16, 68, 18, 82, 11, 59], "raptor_leaf_indices": [4, 6, 7, 8, 10, 11, 13, 16, 17, 18, 19, 20, 22, 23, 33, 35, 37, 39, 41, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64, 68, 72, 74, 75, 76], "baseline_snippets": ["Recent successes in QA, driven largely by the creation of new resources BIBREF2, BIBREF3, BIBREF4, BIBREF5 and advances in model pre-training BIBREF6, BIBREF7, raise a natural question: do state-of-the-art multiple-choice QA (MCQA) models that excel at standard tasks really have basic knowledge and reasoning skills", " Each of our probing datasets consists of multiple-choice questions that include a question $\\textbf {q}$ and a set of answer choices or candidates $\\lbrace a_{1}, a_{N}\\rbrace $  This section describes in detail the 5 different datasets we build, which are drawn from two sources of expert knowledge, namely WordNet BIBREF35 and the GNU Collaborative International Dictionary of English (GCIDE)", "We pre-train on an aggregated training set of the benchmark science exams detailed in Table TABREF21, and created an aggregate development set of around 4k science questions for evaluating overall science performance and inoculation costs  To handle the mismatch between number of answer choices in these sets, we made all sets 5-way by adding empty answers as needed"], "raptor_snippets": [" For example, in measuring competence of definitions, not only do we want to see that the model can handle individual questions such as Figure FIGREF1 1 inside of benchmark tasks, but that it can answer a wider range of questions that exhaustively cover a broad set of concepts and question perturbations (i e , systematic adjustments to how the questions are constructed)  The same applies to ISA reasoning; not only is it important to recognize in the question in Figure FIGREF1", " Our general methodology works as illustrated in Figure FIGREF1: given any MCQA model trained on a set of benchmark tasks, we systematically generate a set of synthetic dataset probes (i e , MCQA renderings of the target information) from information in expert knowledge sources", " We then use these probes to ask two empirical questions: 1) how well do models trained on benchmark tasks perform on these probing tasks and; 2) can such models be re-trained to master new challenges with minimal performance loss on their original tasks While our methodology is amenable to any knowledge source and set of models/benchmark tasks, we focus on probing state-of-the-art transformer models BIBREF7, BIBREF9 in the domain of science MCQA"]}
{"example_idx": 118, "doc_key": "1911.00841", "question": "Are the experts comparable to real-world users?", "overlap_recall": 0.391304347826087, "baseline_ctx_tokens": 1965, "raptor_ctx_tokens": 1928, "leaf_token_frac": 0.7925311203319502, "summary_token_frac": 0.2074688796680498, "baseline_indices": [20, 29, 44, 39, 42, 19, 43, 5, 18, 53, 45, 55, 40, 41, 26, 51, 23, 24, 25, 21, 34, 17, 54], "raptor_node_indices": [31, 5, 27, 24, 34, 33, 18, 1, 23, 19, 6, 3, 28, 26, 25, 21, 12, 2, 32, 4, 13, 8], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], "baseline_snippets": [" In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts Table TABREF14 describes the distribution over first words of questions posed by crowdworkers  We also observe low redundancy in the questions posed by crowdworkers over each policy, with each policy receiving ~49 94 unique questions despite crowdworkers independently posing questions  Questions are on average 8 4 words long", " We consider a disagreement to have occurred when more than one expert does not agree with the majority consensus  By disagreement we mean there is no overlap between the text identified as relevant by one expert and another We find that the annotators agree on the answer for 74% of the questions, even if the supporting evidence they identify is not identical i e full overlap  They disagree on the remaining 26% Experimental Setup", " We collect expert judgments on relevance, subjectivity , silence and information about how likely the question is to be answered from the privacy policy from our experts  We find that most of these mistakes are relevant questions  However many of them were identified as subjective by the annotators, and at least one annotator marked 19 of these questions as having no answer within the privacy policy  However, only 6 of these questions were unexpected or do not usually have an answer in privacy policies"], "raptor_snippets": ["Introduction Privacy policies are the documents which disclose the ways in which a company gathers, uses, shares and manages a user's data  As legal documents, they function using the principle of notice and choice BIBREF0, where companies post their policies, and theoretically, users read the policies and decide to use a company's products or services only if they find the conditions outlined in its privacy policy acceptable", " Many legal jurisdictions around the world accept this framework, including the United States and the European Union BIBREF1, BIBREF2  However, the legitimacy of this framework depends upon users actually reading and understanding privacy policies to determine whether company practices are acceptable to them BIBREF3  In practice this is seldom the case BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10", " This is further complicated by the highly individual and nuanced compromises that users are willing to make with their data BIBREF11, discouraging a `one-size-fits-all' approach to notice of data practices in privacy documents With devices constantly monitoring our environment, including our personal space and our bodies, lack of awareness of how our data is being used easily leads to problematic situations where users are outraged by information misuse, but companies insist that users have consented"]}
{"example_idx": 119, "doc_key": "1911.00841", "question": "Are the answers double (and not triple) annotated?", "overlap_recall": 0.391304347826087, "baseline_ctx_tokens": 1916, "raptor_ctx_tokens": 1941, "leaf_token_frac": 0.7939206594538898, "summary_token_frac": 0.20607934054611024, "baseline_indices": [29, 20, 25, 39, 51, 40, 27, 32, 59, 60, 24, 35, 31, 44, 18, 23, 19, 26, 45, 42, 28, 43, 37], "raptor_node_indices": [25, 26, 33, 34, 31, 5, 27, 3, 24, 23, 18, 12, 0, 32, 6, 1, 22, 28, 19, 14, 17, 4], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28], "baseline_snippets": [" We consider a disagreement to have occurred when more than one expert does not agree with the majority consensus  By disagreement we mean there is no overlap between the text identified as relevant by one expert and another We find that the annotators agree on the answer for 74% of the questions, even if the supporting evidence they identify is not identical i e full overlap  They disagree on the remaining 26% Experimental Setup", " In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts Table TABREF14 describes the distribution over first words of questions posed by crowdworkers  We also observe low redundancy in the questions posed by crowdworkers over each policy, with each policy receiving ~49 94 unique questions despite crowdworkers independently posing questions  Questions are on average 8 4 words long", " We would like to analyze the reasons for potential disagreement on the annotation task, to ensure disagreements arise due to valid differences in opinion rather than lack of adequate specification in annotation guidelines  It is important to note that the annotators are experts rather than crowdworkers  Accordingly, their judgements can be considered valid, legally-informed opinions even when their perspectives differ  For the sake of this question we randomly sample 100 instances in the test data and analyze them for likely reasons for disagreements"], "raptor_snippets": ["Introduction Privacy policies are the documents which disclose the ways in which a company gathers, uses, shares and manages a user's data  As legal documents, they function using the principle of notice and choice BIBREF0, where companies post their policies, and theoretically, users read the policies and decide to use a company's products or services only if they find the conditions outlined in its privacy policy acceptable", " Many legal jurisdictions around the world accept this framework, including the United States and the European Union BIBREF1, BIBREF2  However, the legitimacy of this framework depends upon users actually reading and understanding privacy policies to determine whether company practices are acceptable to them BIBREF3  In practice this is seldom the case BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10", " This is further complicated by the highly individual and nuanced compromises that users are willing to make with their data BIBREF11, discouraging a `one-size-fits-all' approach to notice of data practices in privacy documents With devices constantly monitoring our environment, including our personal space and our bodies, lack of awareness of how our data is being used easily leads to problematic situations where users are outraged by information misuse, but companies insist that users have consented"]}
{"example_idx": 152, "doc_key": "1706.00139", "question": "What is the difference of the proposed model with a standard RNN encoder-decoder?", "overlap_recall": 0.391304347826087, "baseline_ctx_tokens": 1942, "raptor_ctx_tokens": 1932, "leaf_token_frac": 0.7929606625258799, "summary_token_frac": 0.2070393374741201, "baseline_indices": [68, 10, 11, 14, 56, 67, 13, 15, 25, 3, 4, 7, 55, 64, 8, 61, 66, 5, 6, 60, 59, 58, 62], "raptor_node_indices": [24, 36, 22, 27, 10, 6, 37, 3, 34, 4, 30, 32, 23, 19, 11, 5, 21, 35, 25, 26, 29, 28, 9], "raptor_leaf_indices": [3, 4, 5, 6, 7, 9, 10, 11, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32], "baseline_snippets": [" The proposed models empirically show consistent improvement over the previous methods in both the BLEU and ERR evaluation metrics  The proposed models also show an ability to extend to a new, unseen domain no matter how much the in-domain training data was fed  In the future, it would be interesting to apply the proposed model to other tasks that can be modeled based on the encoder-decoder architecture, i e , image captioning, reading comprehension, and machine translation", " The results showed that the model can achieve a satisfactory performance with a small amount of in-domain data by fine tuning the target domain on the out-of-domain trained model More recently, RNN encoder-decoder based models with attention mechanism BIBREF10 have shown improved performances in various tasks  BIBREF12 proposed a review network to the image captioning, which reviews all the information encoded by the encoder and produces a compact thought vector", " BIBREF9 proposed RNN encoder-decoder-based model by using two attention layers to jointly train content selection and surface realization  More close to our work, BIBREF8 proposed an attentive encoder-decoder based generator which computed the attention mechanism over the slot-value pairs  The model showed a domain scalability when a very limited amount of data is available Moving from a limited domain dialogue system to an open domain dialogue system raises some issues"], "raptor_snippets": ["More recently, Encoder-Decoder networks BIBREF6 , BIBREF7 , especially the attentional based models BIBREF8 , BIBREF9 have been explored to solve the NLG tasks  The Attentional RNN Encoder-Decoder BIBREF10 (ARED) based approaches have also shown improved performance on a variety of tasks, e g , image captioning BIBREF11 , BIBREF12 , text summarization BIBREF13 , BIBREF14 ", "While the RNN-based generators with DA gating-vector can prevent the undesirable semantic repetitions, the ARED-based generators show signs of better adapting to a new domain  However, none of the models show significant advantage from out-of-domain data  To better analyze model generalization to an unseen, new domain as well as model leveraging the out-of-domain sources, we propose a new architecture which is an extension of the ARED model", " In order to better select, aggregate and control the semantic information, a Refinement Adjustment LSTM-based component (RALSTM) is introduced to the decoder side  The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences  We conducted experiments on four different NLG domains and found that the proposed methods significantly outperformed the state-of-the-art methods regarding BLEU BIBREF15 and slot error rate ERR scores BIBREF4 "]}
{"example_idx": 146, "doc_key": "1912.13337", "question": "How do they control for annotation artificats?", "overlap_recall": 0.39285714285714285, "baseline_ctx_tokens": 1958, "raptor_ctx_tokens": 1969, "leaf_token_frac": 0.7968511934992382, "summary_token_frac": 0.2031488065007618, "baseline_indices": [20, 61, 19, 24, 44, 58, 40, 41, 43, 6, 59, 65, 23, 48, 8, 31, 18, 25, 91, 115, 15, 100, 28, 70, 83, 39, 69, 46], "raptor_node_indices": [15, 81, 14, 36, 65, 67, 26, 66, 44, 68, 86, 16, 45, 37, 40, 28, 79, 47, 85, 77, 62, 34, 52, 71, 69], "raptor_leaf_indices": [10, 11, 14, 15, 16, 18, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 40, 44, 45, 46, 47, 48, 49, 50, 52, 54, 55, 56, 60, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 77], "baseline_snippets": [" In contrast to this work, we focus on generating data in an entirely automatic fashion, which obviates the need for expensive annotation and gives us the flexibility to construct much larger datasets that control a rich set of semantic aspects of the target questions Dataset Probes and Construction Our probing methodology starts by constructing challenge datasets (Figure FIGREF1, yellow box) from a target set of knowledge resources", "and embed$(\\cdot )$ is an embedding function that assigns token-level embeddings to each token in $s$)", " In the area of MCQA, there is related work on constructing questions from tuples BIBREF32, BIBREF3, both of which involve standard crowd annotation to elicit question-answer pairs (see also BIBREF33, BIBREF34)"], "raptor_snippets": [" This motivates our carefully constructed baselines and close data inspection to ensure probe quality Our results confirm that transformer-based QA models have a remarkable ability to recognize certain types of knowledge captured in our probes—even without additional fine-tuning  Such models can even outperform strong task-specific models trained directly on our probing tasks (e g , on definitions, our best model achieves 77% test accuracy without specialized training, as opposed to 51% for a task-specific LSTM-based model)", " We also show that the same models can be effectively re-fine-tuned on small samples (even 100 examples) of probe data, and that high performance on the probes tends to correlate with a smaller drop in the model's performance on the original QA task Our comprehensive assessment reveals several interesting nuances to the overall positive trend", " Most of this work looks at constructing data through adversarial generation methods, which have also been found useful for creating stronger models BIBREF19  There has also been work on using synthetic data of the type we consider in this paper BIBREF20, BIBREF21, BIBREF22"]}
{"example_idx": 110, "doc_key": "1809.01202", "question": "What baselines did they consider?", "overlap_recall": 0.4, "baseline_ctx_tokens": 1946, "raptor_ctx_tokens": 1996, "leaf_token_frac": 0.8496993987975952, "summary_token_frac": 0.15030060120240482, "baseline_indices": [15, 41, 23, 16, 22, 19, 13, 44, 34, 38, 21, 12, 11, 39, 28, 29, 40, 49, 55, 51, 52, 59, 9, 46, 61], "raptor_node_indices": [43, 11, 51, 42, 13, 9, 67, 45, 34, 36, 66, 64, 41, 57, 53, 8, 40, 35, 24, 50, 32, 38, 29], "raptor_leaf_indices": [8, 9, 11, 13, 24, 29, 32, 34, 35, 36, 37, 38, 40, 41, 42, 43, 45, 50, 51, 53, 54, 55, 56, 57, 58], "baseline_snippets": [" First, Ji and Bhatia used a pretrained model (not fully optimal for some parts of the given task) in their pipeline; Ji's model performed worse than the baseline on the categorization of legislative bills, which is thought to be due to legislative discourse structures differing from those of the training set (WSJ corpus)  Bhatia also used a pretrained model finding that utilizing discourse relation features did not boost accuracy BIBREF4 , BIBREF3 ", " We conducted McNemar's test to determine whether the performance differences are statistically significant at $p <  05$  Results We investigated various models for both causality detection and explanation identification  Based on their performances on the task, we analyzed the relationships between the types of models and the tasks, and scrutinized further for the best performing models  For performance analysis, we reported weighted F1 of classes Causality Prediction", " Finally, we evaluated all of our models on the remaining 10% (Table 1 and Table 2 )  For causal explanation detection task, we extracted discourse arguments using our parser and selected discourse arguments which most cover the annotated causal explanation text span as our gold standard Model We build two types of models  First, we develop feature-based models which utilize features of the successful models in social media analysis and causal relation discourse parsing"], "raptor_snippets": ["The contributions of this work include (1) the proposal of models for both (a) causality prediction and (b) causal explanation identification (2) the extensive evaluation of a variety of models from social media classification models and discourse relation parsers to RNN-based application models demonstrating that feature-based models work best for causality prediction while RNNs are superior for the more difficult task of causal explanation identification (3) performance analysis on architectural differences of the pipeline and the classifier structures", "(4) exploration of the applications of causal explanation to downstream tasks and (5) release of a novel anonymized causality Facebook dataset along with our causality prediction and causal explanation identification models", " The Penn Discourse Treebank (PDTB) BIBREF7 has a `Cause' and `Pragmatic Cause' discourse type under a general `Contingency' class and Rhetorical Structure Theory (RST) BIBREF8 has a `Relations of Cause'  In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse arguments in newswire text (e g"]}
{"example_idx": 168, "doc_key": "2001.02284", "question": "What kind of repetitive and time-consuming activities does their assistant handle?", "overlap_recall": 0.4090909090909091, "baseline_ctx_tokens": 1933, "raptor_ctx_tokens": 1576, "leaf_token_frac": 0.8096446700507615, "summary_token_frac": 0.19035532994923857, "baseline_indices": [40, 0, 4, 42, 8, 65, 22, 13, 48, 1, 79, 50, 37, 10, 34, 12, 95, 3, 96, 2, 56, 80], "raptor_node_indices": [15, 2, 12, 16, 10, 4, 8, 1, 11, 5, 14, 9, 6, 7, 3, 17, 0, 13], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], "baseline_snippets": [" We took real first user questions and predefined possible user responses, as well as gold system answers  These scenarios cover most frequent dialogues that we previously saw in human-to-human dialogue data  We tested our system by implementing a self-chatting evaluation bot  The evaluation cycle could be described as follows: The system receives the first question from a predefined dialogue via an API request, preprocesses and analyzes it in order to extract entities Then it estimates the applicable next action, and responds according to it", "Introduction Robotic Process Automation (RPA) is a type of software bots that simulates hand-operated human activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows BIBREF0  However, one of the drawbacks of RPA-bots is their susceptibility to changes in defined scenarios: being designed for a particular task, the RPA-bot is usually not adaptable to other domains or even light modifications in a workflow BIBREF0", ", answering of FAQs), allowing human workers to focus on more sophisticated inquiries Recent research in the dialogue generation domain is conducted by employing AI-techniques like machine and deep learning BIBREF1, BIBREF2  However, conventional supervised methods have limitations when applied to real-world data and industrial tasks  The primary challenge here refers to a training phase since a robust model requires an extensive amount of structured and labeled data, that is often not available for domain-specific problems"], "raptor_snippets": ["Introduction Robotic Process Automation (RPA) is a type of software bots that simulates hand-operated human activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows BIBREF0  However, one of the drawbacks of RPA-bots is their susceptibility to changes in defined scenarios: being designed for a particular task, the RPA-bot is usually not adaptable to other domains or even light modifications in a workflow BIBREF0", " This inability to readjust to shifting conditions gave rise to Intelligent Process Automation (IPA) systems  IPA-bots combine RPA with Artificial Intelligence (AI) and thus are able to execute more cognitively demanding tasks that require i a  reasoning and language understanding  Hence, IPA-bots advanced beyond automating shallow “click tasks” and can perform jobs more intelligently – by means of machine learning algorithms", " Such IPA-systems undertake time-consuming and routine tasks, and thus enable smart workflows and free up skilled workers to accomplish higher-value activities One of the potential applications of Natural Language Processing (NLP) within the IPA domain are conversational interfaces that enable human-to-machine interaction  The main benefit of conversational systems is their ability to give attention to several users simultaneously while supporting natural communication"]}
{"example_idx": 120, "doc_key": "1911.00841", "question": "Who were the experts used for annotation?", "overlap_recall": 0.4166666666666667, "baseline_ctx_tokens": 1989, "raptor_ctx_tokens": 1950, "leaf_token_frac": 0.7948717948717948, "summary_token_frac": 0.20512820512820512, "baseline_indices": [25, 29, 59, 20, 39, 51, 60, 27, 26, 9, 23, 18, 44, 37, 19, 35, 42, 30, 32, 17, 16, 24, 34, 54], "raptor_node_indices": [33, 24, 31, 25, 27, 5, 34, 3, 18, 1, 28, 26, 6, 23, 2, 0, 8, 9, 29, 12, 11, 21, 32], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": [" We would like to analyze the reasons for potential disagreement on the annotation task, to ensure disagreements arise due to valid differences in opinion rather than lack of adequate specification in annotation guidelines  It is important to note that the annotators are experts rather than crowdworkers  Accordingly, their judgements can be considered valid, legally-informed opinions even when their perspectives differ  For the sake of this question we randomly sample 100 instances in the test data and analyze them for likely reasons for disagreements", " We consider a disagreement to have occurred when more than one expert does not agree with the majority consensus  By disagreement we mean there is no overlap between the text identified as relevant by one expert and another We find that the annotators agree on the answer for 74% of the questions, even if the supporting evidence they identify is not identical i e full overlap  They disagree on the remaining 26% Experimental Setup", " The authors would like to extend their gratitude to Elias Wright, Gian Mascioli, Kiara Pillay, Harrison Kay, Eliel Talo, Alexander Fagella and N  Cameron Russell for providing their valuable expertise and insight to this effort"], "raptor_snippets": ["Introduction Privacy policies are the documents which disclose the ways in which a company gathers, uses, shares and manages a user's data  As legal documents, they function using the principle of notice and choice BIBREF0, where companies post their policies, and theoretically, users read the policies and decide to use a company's products or services only if they find the conditions outlined in its privacy policy acceptable", " Many legal jurisdictions around the world accept this framework, including the United States and the European Union BIBREF1, BIBREF2  However, the legitimacy of this framework depends upon users actually reading and understanding privacy policies to determine whether company practices are acceptable to them BIBREF3  In practice this is seldom the case BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10", " This is further complicated by the highly individual and nuanced compromises that users are willing to make with their data BIBREF11, discouraging a `one-size-fits-all' approach to notice of data practices in privacy documents With devices constantly monitoring our environment, including our personal space and our bodies, lack of awareness of how our data is being used easily leads to problematic situations where users are outraged by information misuse, but companies insist that users have consented"]}
{"example_idx": 129, "doc_key": "1910.12574", "question": "What are the existing biases?", "overlap_recall": 0.4166666666666667, "baseline_ctx_tokens": 1949, "raptor_ctx_tokens": 1937, "leaf_token_frac": 0.7418688693856479, "summary_token_frac": 0.2581311306143521, "baseline_indices": [68, 20, 56, 41, 49, 26, 10, 7, 34, 13, 67, 55, 29, 5, 57, 50, 27, 17, 65, 8, 51, 63, 9, 40], "raptor_node_indices": [53, 7, 48, 35, 12, 36, 29, 6, 11, 47, 46, 50, 49, 34, 16, 8, 43, 15, 39, 44, 3, 28, 19], "raptor_leaf_indices": [2, 3, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], "baseline_snippets": [" It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data", " Furthermore, researchers have recently focused on the bias derived from the hate speech training datasets BIBREF18, BIBREF2, BIBREF19  Davidson et al  BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection  Wiegand et al", " A large majority of the errors come from misclassifying hateful categories (racism and sexism) as hatless (neither) and vice versa  0 9% and 18 5% of all racism samples are misclassified as sexism and neither respectively whereas it is 0% and 12 7% for sexism samples  Almost 12% of neither samples are misclassified as racism or sexism"], "raptor_snippets": [" Therefore, governments and social network platforms confronting the trend must have tools to detect aggressive behavior in general, and hate speech in particular, as these forms of online aggression not only poison the social climate of the online communities that experience it, but can also provoke physical violence and serious harm BIBREF1 Recently, the problem of online abusive detection has attracted scientific attention", " Proof of this is the creation of the third Workshop on Abusive Language Online or Kaggle’s Toxic Comment Classification Challenge that gathered 4,551 teams in 2018 to detect different types of toxicities (threats, obscenity, etc )  In the scope of this work, we mainly focus on the term hate speech as abusive content in social media, since it can be considered a broad umbrella term for numerous kinds of insulting user-generated content", " Although supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment analysis, lexical resources, linguistic features, knowledge-based features or user-based and platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach  The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers"]}
{"example_idx": 156, "doc_key": "1912.00667", "question": "How are the accuracy merits of the approach demonstrated?", "overlap_recall": 0.4166666666666667, "baseline_ctx_tokens": 1991, "raptor_ctx_tokens": 1940, "leaf_token_frac": 0.8969072164948454, "summary_token_frac": 0.10309278350515463, "baseline_indices": [58, 11, 55, 70, 54, 16, 69, 78, 6, 42, 45, 51, 63, 10, 44, 49, 5, 67, 7, 68, 60, 13, 32, 30], "raptor_node_indices": [60, 12, 75, 68, 84, 73, 41, 21, 40, 87, 57, 64, 11, 50, 55, 23, 74, 70, 65, 51, 13, 54], "raptor_leaf_indices": [11, 12, 13, 16, 21, 23, 40, 41, 43, 50, 51, 54, 55, 57, 58, 59, 60, 61, 64, 65, 68, 70, 73, 74, 75], "baseline_snippets": ["17% (Accuracy) and 18 38% (AUC), and MLP by 10 71% (Accuracy) and 30 27% (AUC) on average  Such significant improvements clearly demonstrate that our approach is effective at improving model performance  We observe that the target models generally converge between the 7th and 9th iteration on both datasets when performance is measured by AUC", "An extensive empirical evaluation of our approach on multiple real-world datasets demonstrating that our approach significantly improves the state of the art by an average of 24 3% AUC The rest of this paper is organized as follows  First, we present our human-AI loop approach in Section SECREF2  Subsequently, we introduce our proposed probabilistic model in Section SECREF3  The experimental setup and results are presented in Section SECREF4", " We note that due to the imbalance in our datasets (20% positive microposts in CyberAttack and 27% in PoliticianDeath), accuracy is dominated by negative examples; AUC, in comparison, better characterizes the discriminative power of the model Crowdsourcing  We chose Level 3 workers on the Figure-Eight crowdsourcing platform for our experiments  The inter-annotator agreement in micropost classification is taken into account through the EM algorithm"], "raptor_snippets": ["An extensive empirical evaluation of our approach on multiple real-world datasets demonstrating that our approach significantly improves the state of the art by an average of 24 3% AUC The rest of this paper is organized as follows  First, we present our human-AI loop approach in Section SECREF2  Subsequently, we introduce our proposed probabilistic model in Section SECREF3  The experimental setup and results are presented in Section SECREF4", " Finally, we briefly cover related work in Section SECREF5 before concluding our work in Section SECREF6 The Human-AI Loop Approach Given a set of labeled and unlabeled microposts, our goal is to extract informative keywords and estimate their expectations in order to train a machine learning model  To achieve this goal, our proposed human-AI loop approach comprises two crowdsourcing tasks, i e", ", micropost classification followed by keyword discovery, and a unified probabilistic model for expectation inference and model training  Figure FIGREF6 presents an overview of our approach  Next, we describe our approach from a process-centric perspective"]}
{"example_idx": 159, "doc_key": "1912.00667", "question": "How are the interpretability merits of the approach demonstrated?", "overlap_recall": 0.4166666666666667, "baseline_ctx_tokens": 1950, "raptor_ctx_tokens": 1985, "leaf_token_frac": 0.7984886649874056, "summary_token_frac": 0.20151133501259447, "baseline_indices": [76, 11, 7, 78, 13, 45, 12, 70, 69, 5, 16, 6, 51, 63, 75, 77, 22, 10, 8, 26, 58, 57, 67, 49], "raptor_node_indices": [75, 60, 84, 85, 24, 28, 76, 21, 78, 74, 82, 23, 44, 70, 36, 10, 72, 57, 38, 40, 68, 22, 25], "raptor_leaf_indices": [7, 8, 10, 17, 21, 22, 23, 24, 25, 28, 32, 36, 38, 40, 41, 42, 44, 45, 46, 48, 49, 50, 52, 56, 57, 59, 60, 61, 68, 70, 72, 74, 75, 76], "baseline_snippets": [" Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability  In contrast, our approach directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model Conclusion", "An extensive empirical evaluation of our approach on multiple real-world datasets demonstrating that our approach significantly improves the state of the art by an average of 24 3% AUC The rest of this paper is organized as follows  First, we present our human-AI loop approach in Section SECREF2  Subsequently, we introduce our proposed probabilistic model in Section SECREF3  The experimental setup and results are presented in Section SECREF4", " By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10  An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks  Such an advantage is particularly useful for event detection using complex models, e g"], "raptor_snippets": [" By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10  An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks  Such an advantage is particularly useful for event detection using complex models, e g", ", deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12  An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13  In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions", "To the best of our knowledge, we are the first to propose a human-AI loop approach that iteratively improves machine learning models for event detection  In summary, our work makes the following key contributions: A novel human-AI loop approach for micropost event detection that jointly discovers informative keywords and estimates their expectation; A unified probabilistic model that infers keyword expectation and simultaneously performs model training;"]}
{"example_idx": 173, "doc_key": "1909.03582", "question": "How is sensationalism scorer trained?", "overlap_recall": 0.45454545454545453, "baseline_ctx_tokens": 1920, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [11, 7, 4, 18, 72, 71, 12, 63, 50, 27, 49, 33, 6, 2, 30, 16, 55, 8, 15, 3, 54, 35], "raptor_node_indices": [15, 9, 19, 20, 5, 12, 13, 8, 2, 18, 6, 4, 1, 14, 3, 17, 7, 21, 11, 10, 0, 16], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" We then train the sensationalism scorer by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text{sen}}$  Firstly, 1-D convolution is used to extract word features from the input embeddings of a headline  This is followed by a ReLU activation layer and a max-pooling layer along the time dimension", " 2) Without human-annotated data, we propose a distant supervision strategy to train a sensationalism scorer as a reward function 3) We propose a novel loss function, Auto-tuned Reinforcement Learning, to give dynamic weights to balance between MLE and RL  Our code will be released  Sensationalism Scorer To evaluate the sensationalism intensity score $\\alpha _{\\text{sen}}$ of a headline, we collect a sensationalism dataset and then train a sensationalism scorer", " Experimental results show that by distinguishing these two types of headlines, we can partially teach the model a sense of being sensational Secondly, after training a sensationalism scorer on our sensationalism dataset, a natural way to generate sensational headlines is to maximize the sensationalism score using reinforcement learning (RL)  However, the following shows an example of a RL model maximizing the sensationalism score by generating a very unnatural sentence, while its sensationalism scorer gave a very high score of 0"], "raptor_snippets": ["Introduction Headline generation is the process of creating a headline-style sentence given an input article  The research community has been regarding the task of headline generation as a summarization task BIBREF1, ignoring the fundamental differences between headlines and summaries  While summaries aim to contain most of the important information from the articles, headlines do not necessarily need to  Instead, a good headline needs to capture people's attention and serve as an irresistible invitation for users to read through the article", " For example, the headline “$2 Billion Worth of Free Media for Trump”, which gives only an intriguing hint, is considered better than the summarization style headline “Measuring Trump’s Media Dominance” , as the former gets almost three times the readers as the latter  Generating headlines with many clicks is especially important in this digital age, because many of the revenues of journalism come from online advertisements and getting more user clicks means being more competitive in the market", " However, most existing websites naively generate sensational headlines using only keywords or templates  Instead, this paper aims to learn a model that generates sensational headlines based on an input article without labeled data To generate sensational headlines, there are two main challenges  Firstly, there is a lack of sensationalism scorer to measure how sensational a headline is  Some researchers have tried to manually label headlines as clickbait or non-clickbait BIBREF2, BIBREF3"]}
{"example_idx": 160, "doc_key": "1912.00667", "question": "Do they report results only on English data?", "overlap_recall": 0.4583333333333333, "baseline_ctx_tokens": 1953, "raptor_ctx_tokens": 1966, "leaf_token_frac": 0.8982706002034588, "summary_token_frac": 0.1017293997965412, "baseline_indices": [60, 57, 46, 76, 48, 62, 55, 64, 59, 69, 63, 47, 23, 21, 61, 26, 51, 5, 54, 2, 58, 75, 24, 70], "raptor_node_indices": [84, 68, 57, 38, 55, 72, 16, 75, 76, 12, 18, 13, 59, 17, 61, 47, 11, 41, 60, 65, 37, 87, 73], "raptor_leaf_indices": [11, 12, 13, 16, 17, 18, 21, 37, 38, 40, 41, 43, 47, 51, 55, 57, 58, 59, 60, 61, 65, 68, 72, 73, 75, 76], "baseline_snippets": [" For instance, for the CyberAttack dataset the new keyword in the 9th iteration `election' frequently co-occurs with the keyword `russia' in the 5th iteration (in microposts that connect Russian hackers with US elections), thus bringing limited new information for improving the model performance  As a side remark, we note that the models converge faster when performance is measured by accuracy", ", CyberAttack), our approach only requires to manually inspect 800 tweets (for 8 keywords), which is only 1% of the entire dataset Experiments and Results ::: Results of our Human-AI Loop (Q1) Table TABREF26 reports the evaluation of our approach on both the CyberAttack and PoliticianDeath event categories  Our approach is configured such that each iteration starts with 1 new keyword discovered in the previous iteration Our approach improves LR by 5", "Experiments and Results ::: Experimental Setup Datasets  We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath)  These event categories are chosen as they are representative of important event types that are of interest to many governments and companies  The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts  The few available datasets do not suit our requirements"], "raptor_snippets": ["An extensive empirical evaluation of our approach on multiple real-world datasets demonstrating that our approach significantly improves the state of the art by an average of 24 3% AUC The rest of this paper is organized as follows  First, we present our human-AI loop approach in Section SECREF2  Subsequently, we introduce our proposed probabilistic model in Section SECREF3  The experimental setup and results are presented in Section SECREF4", " Finally, we briefly cover related work in Section SECREF5 before concluding our work in Section SECREF6 The Human-AI Loop Approach Given a set of labeled and unlabeled microposts, our goal is to extract informative keywords and estimate their expectations in order to train a machine learning model  To achieve this goal, our proposed human-AI loop approach comprises two crowdsourcing tasks, i e", ", micropost classification followed by keyword discovery, and a unified probabilistic model for expectation inference and model training  Figure FIGREF6 presents an overview of our approach  Next, we describe our approach from a process-centric perspective"]}
{"example_idx": 56, "doc_key": "1909.09587", "question": "what does the model learn in zero-shot setting?", "overlap_recall": 0.4782608695652174, "baseline_ctx_tokens": 1968, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [22, 26, 28, 13, 24, 8, 6, 3, 5, 20, 7, 36, 19, 14, 31, 11, 12, 17, 9, 33, 37, 23, 38], "raptor_node_indices": [9, 13, 2, 19, 5, 12, 20, 18, 10, 6, 1, 15, 3, 21, 8, 17, 0, 16, 14, 11, 7, 4], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" Therefore, although translation degrades the performance, whether translating the corpus into the target language is not critical What Does Zero-shot Transfer Model Learn  ::: Unseen Language Dataset It has been shown that extractive QA tasks like SQuAD may be tackled by some language independent strategies, for example, matching words in questions and context BIBREF20  Is zero-shot learning feasible because the model simply learns this kind of language independent strategies on one language and apply to the other", " The results can be found in the Appendix What Does Zero-shot Transfer Model Learn  ::: Code-switching Dataset We observe linguistic-agnostic representations in the last subsection  If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data  Because there is no code-switching data for RC, we create artificial code-switching datasets by replacing some of the words in contexts or questions with their synonyms in another language", " However, the examples of the answers of the model (Table TABREF21) show that multi-BERT could find the correct answer spans although some keywords in the spans have been translated into another language What Does Zero-shot Transfer Model Learn  ::: Typology-manipulated Dataset There are various types of typology in languages"], "raptor_snippets": ["Introduction Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries", " In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", " The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance  Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data"]}
{"example_idx": 64, "doc_key": "1911.10049", "question": "How larger are the training sets of these versions of ELMo compared to the previous ones?", "overlap_recall": 0.4782608695652174, "baseline_ctx_tokens": 1950, "raptor_ctx_tokens": 1909, "leaf_token_frac": 0.8428496595075956, "summary_token_frac": 0.1571503404924044, "baseline_indices": [11, 49, 12, 16, 14, 38, 15, 45, 25, 48, 36, 46, 47, 40, 50, 5, 26, 8, 23, 35, 43, 34, 10], "raptor_node_indices": [23, 19, 14, 21, 31, 28, 29, 22, 5, 30, 27, 8, 33, 17, 10, 18, 13, 12, 2, 15, 20, 4], "raptor_leaf_indices": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29], "baseline_snippets": ["Although ELMo is trained on character level and is able to handle out-of-vocabulary words, a vocabulary file containing most common tokens is used for efficiency during training and embedding generation  The original ELMo model was trained on a one billion word large English corpus, with a given vocabulary file of about 800,000 words  Later, ELMo models for other languages were trained as well, but limited to larger languages with many resources, like German and Japanese", " We present the necessary background on embeddings and contextual embeddings, the details of training the embedding models, and their evaluation  We show that the size of used training sets importantly affects the quality of produced embeddings, and therefore the existing publicly available ELMo embeddings for the processed languages are inadequate  We trained new ELMo embeddings on larger training sets and analysed their properties on the analogy task and on the NER task", "ELMo ::: ELMoForManyLangs Recently, ELMoForManyLangs BIBREF6 project released pre-trained ELMo models for a number of different languages BIBREF7  These models, however, were trained on a significantly smaller datasets"], "raptor_snippets": [" During training of an embedding, all senses of a given word (e g , paper as a material, as a newspaper, as a scientific work, and as an exam) contribute relevant information in proportion to their frequency in the training corpus  This causes the final vector to be placed somewhere in the weighted middle of all words' meanings  Consequently, rare meanings of words are poorly expressed with word2vec and the resulting vectors do not offer good semantic representations", " For example, none of the 50 closest vectors of the word paper is related to science The idea of contextual embeddings is to generate a different vector for each context a word appears in and the context is typically defined sentence-wise  To a large extent, this solves the problems with word polysemy, i e  the context of a sentence is typically enough to disambiguate different meanings of a word for humans and so it is for the learning algorithms", " In this work, we describe high-quality models for contextual embeddings, called ELMo BIBREF3, precomputed for seven morphologically rich, less-resourced languages: Slovenian, Croatian, Finnish, Estonian, Latvian, Lithuanian, and Swedish  ELMo is one of the most successful approaches to contextual word embeddings"]}
{"example_idx": 67, "doc_key": "1910.10781", "question": "On top of BERT does the RNN layer work better or the transformer layer?", "overlap_recall": 0.4782608695652174, "baseline_ctx_tokens": 1962, "raptor_ctx_tokens": 1964, "leaf_token_frac": 0.8981670061099797, "summary_token_frac": 0.10183299389002037, "baseline_indices": [10, 18, 13, 12, 15, 17, 38, 31, 0, 28, 5, 4, 9, 29, 30, 19, 39, 14, 33, 37, 36, 11, 34], "raptor_node_indices": [17, 45, 49, 51, 26, 13, 46, 42, 25, 28, 27, 4, 29, 14, 36, 24, 15, 34, 21, 35, 19, 18, 44, 16], "raptor_leaf_indices": [4, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 42, 44, 45, 46], "baseline_snippets": [" BERT is built upon the Transformer architecture BIBREF0, which uses self-attention, feed-forward layers, residual connections and layer normalization as the main building blocks  It has two pre-training objectives: Masked language modelling - some of the words in a sentence are being masked and the model has to predict them based on the context (note the difference from the typical autoregressive language model training objective);", "Given that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc )", " BERT-Base and BERT-Large are different in model parameters such as number of transformer blocks, number of self-attention heads  Total number of parameters in BERT-Base are 110M and 340M in BERT-Large BERT suffers from major limitations in terms of handling long sequences  Firstly, the self-attention layer has a quadratic complexity $O(n^2)$ in terms of the sequence length $n$ BIBREF0"], "raptor_snippets": [" We split the input text sequence into shorter segments in order to obtain a representation for each of them using BERT  Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification  We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT)  Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers", " BERT-Base and BERT-Large are different in model parameters such as number of transformer blocks, number of self-attention heads  Total number of parameters in BERT-Base are 110M and 340M in BERT-Large BERT suffers from major limitations in terms of handling long sequences  Firstly, the self-attention layer has a quadratic complexity $O(n^2)$ in terms of the sequence length $n$ BIBREF0", " Secondly, BERT uses a learned positional embeddings scheme BIBREF1, which means that it won't likely be able to generalize to positions beyond those seen in the training data To investigate the effect of fine-tuning BERT on task performance, we use either the pre-trained BERT weights, or the weights from a BERT fine-tuned on the task-specific dataset on a segment-level (i e"]}
{"example_idx": 53, "doc_key": "1909.09587", "question": "What is the model performance on target language reading comprehension?", "overlap_recall": 0.5, "baseline_ctx_tokens": 1971, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [17, 0, 5, 16, 21, 29, 14, 18, 19, 13, 28, 2, 22, 30, 31, 12, 6, 15, 36, 4, 32, 20, 11, 27], "raptor_node_indices": [15, 12, 13, 11, 19, 4, 18, 5, 2, 10, 20, 6, 7, 3, 17, 1, 9, 21, 16, 14, 0, 8], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53 8, while the F1 score is only 44 1 for the model training on Zh-En  This shows that translation degrades the quality of data  There are some exceptions when testing on Korean  Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean", "Introduction Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries", "In this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC  We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT"], "raptor_snippets": ["Introduction Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries", " In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", " The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance  Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data"]}
{"example_idx": 54, "doc_key": "1909.09587", "question": "What source-target language pairs were used in this work? ", "overlap_recall": 0.5217391304347826, "baseline_ctx_tokens": 1960, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [5, 12, 27, 26, 28, 15, 30, 10, 6, 36, 22, 8, 9, 20, 11, 31, 14, 7, 34, 4, 37, 23, 16], "raptor_node_indices": [8, 18, 4, 6, 12, 7, 17, 9, 15, 20, 2, 13, 5, 19, 16, 1, 14, 21, 11, 3, 10, 0], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": ["In this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC  We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT", " Data in different languages were simply mixed in batches while pre-training, without additional effort to align between languages  When fine-tuning, we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged Zero-shot Transfer with Multi-BERT ::: Experimental Results Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese", " The synonyms are found by word-by-word translation with given dictionaries  We use the bilingual dictionaries collected and released in facebookresearch/MUSE GitHub repository  We substitute the words if and only if the words are in the bilingual dictionaries Table TABREF14 shows that on all the code-switching datasets, the EM/F1 score drops, indicating that the semantics of representations are not totally disentangled from language"], "raptor_snippets": ["Introduction Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries", " In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", " The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance  Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data"]}
{"example_idx": 153, "doc_key": "1706.00139", "question": "Does the model evaluated on NLG datasets or dialog datasets?", "overlap_recall": 0.5217391304347826, "baseline_ctx_tokens": 1980, "raptor_ctx_tokens": 1999, "leaf_token_frac": 0.7998999499749875, "summary_token_frac": 0.2001000500250125, "baseline_indices": [49, 0, 55, 1, 50, 35, 62, 6, 11, 63, 12, 7, 3, 9, 67, 4, 45, 5, 52, 2, 48, 58, 16], "raptor_node_indices": [36, 3, 4, 2, 24, 5, 11, 19, 38, 8, 26, 32, 6, 18, 9, 12, 16, 7, 1, 20, 35, 37, 30], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24, 26, 27, 28, 29, 30, 32], "baseline_snippets": ["We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television  The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs", "Introduction Natural Language Generation (NLG) plays a critical role in Spoken Dialogue Systems (SDS) with task is to convert a meaning representation produced by the Dialogue Manager into natural language utterances  Conventional approaches still rely on comprehensive hand-tuning templates and rules requiring expert knowledge of linguistic representation, including rule-based BIBREF0 , corpus-based n-gram models BIBREF1 , and a trainable generator BIBREF2 ", " We compared the proposed models against three strong baselines which have been recently published as state-of-the-art NLG benchmarks[]  https://github com/shawnwun/RNNLG HLSTM proposed by BIBREF3 thwsjy15 which used a heuristic gate to ensure that all of the slot-value information was accurately captured when generating SCLSTM proposed by BIBREF4 wensclstm15 which can jointly learn the gating signal and language model"], "raptor_snippets": ["Introduction Natural Language Generation (NLG) plays a critical role in Spoken Dialogue Systems (SDS) with task is to convert a meaning representation produced by the Dialogue Manager into natural language utterances  Conventional approaches still rely on comprehensive hand-tuning templates and rules requiring expert knowledge of linguistic representation, including rule-based BIBREF0 , corpus-based n-gram models BIBREF1 , and a trainable generator BIBREF2 ", "Recently, Recurrent Neural Networks (RNNs) based approaches have shown promising performance in tackling the NLG problems  The RNN-based models have been applied for NLG as a joint training model BIBREF3 , BIBREF4 and an end-to-end training model BIBREF5   A recurring problem in such systems is requiring annotated datasets for particular dialogue acts (DAs)", " To ensure that the generated utterance representing the intended meaning of the given DA, the previous RNN-based models were further conditioned on a 1-hot vector representation of the DA  BIBREF3 introduced a heuristic gate to ensure that all the slot-value pair was accurately captured during generation  BIBREF4 subsequently proposed a Semantically Conditioned Long Short-term Memory generator (SC-LSTM) which jointly learned the DA gating signal and language model"]}
{"example_idx": 158, "doc_key": "1912.00667", "question": "How is the keyword specific expectation elicited from the crowd?", "overlap_recall": 0.5416666666666666, "baseline_ctx_tokens": 1929, "raptor_ctx_tokens": 1960, "leaf_token_frac": 0.7448979591836735, "summary_token_frac": 0.25510204081632654, "baseline_indices": [31, 6, 5, 75, 38, 77, 27, 39, 69, 44, 63, 3, 13, 9, 21, 4, 23, 22, 25, 12, 45, 34, 40, 51], "raptor_node_indices": [86, 33, 29, 78, 26, 32, 18, 85, 22, 24, 45, 6, 48, 49, 37, 5, 69, 75, 34, 10, 28, 79, 40, 81], "raptor_leaf_indices": [5, 6, 10, 11, 14, 15, 18, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 42, 44, 45, 46, 47, 48, 49, 50, 52, 54, 55, 56, 60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75], "baseline_snippets": [" Our goal is to infer the keyword-specific expectation $e^{(t)}$ and train the target model by learning the model parameter $\\theta ^{(t)}$  An additional parameter of our probabilistic model is the reliability of crowd workers, which is essential when involving crowdsourcing", " More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement  By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance", "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably  Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords"], "raptor_snippets": ["To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably  Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords", " More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement  By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance", "To the best of our knowledge, we are the first to propose a human-AI loop approach that iteratively improves machine learning models for event detection  In summary, our work makes the following key contributions: A novel human-AI loop approach for micropost event detection that jointly discovers informative keywords and estimates their expectation; A unified probabilistic model that infers keyword expectation and simultaneously performs model training;"]}
{"example_idx": 58, "doc_key": "2002.08795", "question": "What are the baselines?", "overlap_recall": 0.56, "baseline_ctx_tokens": 1953, "raptor_ctx_tokens": 1985, "leaf_token_frac": 0.8992443324937027, "summary_token_frac": 0.10075566750629723, "baseline_indices": [38, 6, 26, 37, 3, 35, 2, 19, 28, 4, 34, 5, 21, 15, 29, 14, 33, 16, 20, 23, 13, 9, 32, 18, 27], "raptor_node_indices": [21, 20, 18, 32, 23, 19, 22, 9, 14, 13, 1, 11, 16, 24, 7, 8, 6, 15, 2, 12, 5, 17, 30, 10], "raptor_leaf_indices": [1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], "baseline_snippets": [" Cell step size is a parameter used for Go-Explore and describes how many steps are taken when exploring in a given cell state  Base hyperparameters for KG-A2C are taken from BIBREF6 and the same parameters are used for A2C", " Zork1 has 237 templates, each with up to two blanks, yielding a template-action space of size $\\mathcal {O}(237 \\times 697^2)={1 15e8}$  This space is still far larger than most used by previous approaches applying reinforcement learning to text-based games State-Representation", "We compare our two exploration strategies to the following baselines and ablations: KG-A2C This is the exact same method presented in BIBREF6 with no modifications A2C Represents the same approach as KG-A2C but with all the knowledge graph components removed  The state representation is text only encoded using recurrent networks"], "raptor_snippets": [" Text-adventure games provide us with multiple challenges in the form of partial observability, commonsense reasoning, and a combinatorially-sized state-action space  Text-adventure games are structured as long puzzles or quests, interspersed with bottlenecks  The quests can usually be completed through multiple branching paths  However, games can also feature one or more bottlenecks", " Bottlenecks are areas that an agent must pass through in order to progress to the next section of the game regardless of what path the agent has taken to complete that section of the quest BIBREF0  In this work, we focus on more effectively exploring this space and surpassing these bottlenecks—building on prior work that focuses on tackling the other problems Formally, we use the definition of text-adventure games as seen in BIBREF1 and BIBREF2", " To facilitate text-adventure game playing, BIBREF2 introduce Jericho, a framework for interacting with text-games  They propose a template-based action space in which the agent first selects a template, consisting of an action verb and preposition, and then filling that in with relevant entities $($e g  $[get]$ $ [from] $ $)$"]}
{"example_idx": 126, "doc_key": "1910.12574", "question": "Which publicly available datasets are used?", "overlap_recall": 0.56, "baseline_ctx_tokens": 1949, "raptor_ctx_tokens": 1967, "leaf_token_frac": 0.7966446365022878, "summary_token_frac": 0.20335536349771224, "baseline_indices": [20, 13, 6, 12, 5, 42, 21, 19, 40, 48, 11, 23, 22, 41, 61, 7, 47, 50, 57, 17, 68, 26, 2, 25, 15], "raptor_node_indices": [9, 11, 8, 37, 48, 50, 5, 3, 36, 32, 0, 7, 10, 51, 41, 47, 12, 33, 4, 25, 35, 40, 13], "raptor_leaf_indices": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18, 19, 20, 21, 22, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41], "baseline_snippets": [" Furthermore, researchers have recently focused on the bias derived from the hate speech training datasets BIBREF18, BIBREF2, BIBREF19  Davidson et al  BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection  Wiegand et al", " Apart from features, different algorithms such as Support Vector Machines BIBREF15, Naive Baye BIBREF1, and Logistic Regression BIBREF5, BIBREF9, etc  have been applied for classification purposes  Waseem et al  BIBREF5 provided a test with a list of criteria based on the work in Gender Studies and Critical Race Theory (CRT) that can annotate a corpus of more than $16k$ tweets as racism, sexism, or neither", " Although supervised machine learning-based approaches have used different text mining-based features such as surface features, sentiment analysis, lexical resources, linguistic features, knowledge-based features or user-based and platform-based metadata BIBREF8, BIBREF9, BIBREF10, they necessitate a well-defined feature extraction approach  The trend now seems to be changing direction, with deep learning models being used for both feature extraction and the training of classifiers"], "raptor_snippets": ["Introduction People are increasingly using social networking platforms such as Twitter, Facebook, YouTube, etc  to communicate their opinions and share information  Although the interactions among users on these platforms can lead to constructive conversations, they have been increasingly exploited for the propagation of abusive language and the organization of hate-based activities BIBREF0, BIBREF1, especially due to the mobility and anonymous environment of these online platforms  Violence attributed to online hate speech has increased worldwide", " Therefore, governments and social network platforms confronting the trend must have tools to detect aggressive behavior in general, and hate speech in particular, as these forms of online aggression not only poison the social climate of the online communities that experience it, but can also provoke physical violence and serious harm BIBREF1 Recently, the problem of online abusive detection has attracted scientific attention", " Proof of this is the creation of the third Workshop on Abusive Language Online or Kaggle’s Toxic Comment Classification Challenge that gathered 4,551 teams in 2018 to detect different types of toxicities (threats, obscenity, etc )  In the scope of this work, we mainly focus on the term hate speech as abusive content in social media, since it can be considered a broad umbrella term for numerous kinds of insulting user-generated content"]}
{"example_idx": 55, "doc_key": "1909.09587", "question": "What model is used as a baseline?  ", "overlap_recall": 0.5652173913043478, "baseline_ctx_tokens": 1913, "raptor_ctx_tokens": 1840, "leaf_token_frac": 0.782608695652174, "summary_token_frac": 0.21739130434782608, "baseline_indices": [1, 9, 2, 26, 28, 12, 13, 3, 33, 30, 5, 37, 6, 17, 38, 8, 22, 11, 4, 20, 15, 31, 29], "raptor_node_indices": [9, 2, 20, 17, 19, 4, 5, 10, 6, 12, 8, 21, 13, 15, 18, 0, 3, 16, 14, 1, 7, 11], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "baseline_snippets": [" In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", " The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD  We always use the development sets of SQuAD, DRCD and KorQuAD for testing since the testing sets of the corpora have not been released yet", " The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance  Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data"], "raptor_snippets": ["Introduction Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries", " In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", " The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance  Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data"]}
{"example_idx": 117, "doc_key": "1911.02855", "question": "How are weights dynamically adjusted?", "overlap_recall": 0.5652173913043478, "baseline_ctx_tokens": 1977, "raptor_ctx_tokens": 1984, "leaf_token_frac": 0.7479838709677419, "summary_token_frac": 0.25201612903225806, "baseline_indices": [9, 12, 14, 13, 21, 22, 49, 17, 36, 31, 15, 42, 32, 33, 61, 4, 0, 26, 59, 35, 27, 45, 16], "raptor_node_indices": [17, 15, 13, 30, 32, 12, 22, 26, 16, 9, 5, 31, 29, 2, 14, 4, 20, 10, 19, 18, 34, 11, 33], "raptor_leaf_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], "baseline_snippets": [" Inspired by the idea of focal loss BIBREF16 in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds  This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples", " Experimental results are presented in Section 4  We perform ablation studies in Section 5, followed by a brief conclusion in Section 6 Related Work ::: Data Resample The idea of weighting training examples has a long history  Importance sampling BIBREF17 assigns weights to different samples and changes the data distribution  Boosting algorithms such as AdaBoost BIBREF18 select harder examples to train subsequent classifiers", " In self-paced learning BIBREF22, example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first  At each training step, self-paced learning algorithm optimizes model parameters and example weights jointly  Other works BIBREF23, BIBREF24 adjusted the weights of different training examples based on training loss  Besides, recent work BIBREF25, BIBREF26 proposed to learn a separate network to predict sample weights"], "raptor_snippets": ["Introduction Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension  Table TABREF3 gives concrete examples: for the Named Entity Recognition (NER) task BIBREF2, BIBREF3, most tokens are backgrounds with tagging class $O$  Specifically, the number of tokens tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5", "0 dataset; Data-imbalanced issue is more severe for MRC tasks BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 with the value of negative-positive ratio being 50-200 Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label", " This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function while at test time, F1 score concerns more about positive examples; (2) the overwhelming effect of easy-negative examples  As pointed out by meng2019dsreg, significantly large number of negative examples also means that the number of easy-negative example is large"]}
{"example_idx": 157, "doc_key": "1912.00667", "question": "What type of classifiers are used?", "overlap_recall": 0.5652173913043478, "baseline_ctx_tokens": 1951, "raptor_ctx_tokens": 1931, "leaf_token_frac": 0.6892801657172449, "summary_token_frac": 0.31071983428275507, "baseline_indices": [13, 21, 15, 27, 49, 56, 25, 18, 2, 26, 72, 17, 62, 68, 55, 38, 52, 3, 10, 44, 1, 51, 75], "raptor_node_indices": [33, 84, 11, 85, 44, 42, 78, 79, 52, 38, 87, 60, 62, 25, 10, 80, 71, 47, 37, 75, 59, 66, 50], "raptor_leaf_indices": [10, 11, 12, 16, 19, 20, 21, 24, 25, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 73, 75], "baseline_snippets": [", micropost classification followed by keyword discovery, and a unified probabilistic model for expectation inference and model training  Figure FIGREF6 presents an overview of our approach  Next, we describe our approach from a process-centric perspective", " As output, it generates a keyword-specific expectation, denoted as $e^{(t)}$, and an improved version of the micropost classification model, denoted as $p_{\\theta ^{(t)}}(y|x)$  The details of our probabilistic model are given in Section SECREF3 Keyword Discovery  The keyword discovery task aims at discovering a new keyword (or a set of keywords) that is most informative for model training with respect to existing keywords", ", positively labeled microposts) $\\mathcal {L}$, post-filtering with a list of seed events  $\\mathcal {U}$ and $\\mathcal {L}$ are used together to train a discriminative model (e g , a deep neural network) for classifying the relevance of microposts to an event"], "raptor_snippets": ["To the best of our knowledge, we are the first to propose a human-AI loop approach that iteratively improves machine learning models for event detection  In summary, our work makes the following key contributions: A novel human-AI loop approach for micropost event detection that jointly discovers informative keywords and estimates their expectation; A unified probabilistic model that infers keyword expectation and simultaneously performs model training;", "An extensive empirical evaluation of our approach on multiple real-world datasets demonstrating that our approach significantly improves the state of the art by an average of 24 3% AUC The rest of this paper is organized as follows  First, we present our human-AI loop approach in Section SECREF2  Subsequently, we introduce our proposed probabilistic model in Section SECREF3  The experimental setup and results are presented in Section SECREF4", " Finally, we briefly cover related work in Section SECREF5 before concluding our work in Section SECREF6 The Human-AI Loop Approach Given a set of labeled and unlabeled microposts, our goal is to extract informative keywords and estimate their expectations in order to train a machine learning model  To achieve this goal, our proposed human-AI loop approach comprises two crowdsourcing tasks, i e"]}
{"example_idx": 111, "doc_key": "1809.01202", "question": "What types of social media did they consider?", "overlap_recall": 0.5833333333333334, "baseline_ctx_tokens": 1940, "raptor_ctx_tokens": 1929, "leaf_token_frac": 0.8444790046656299, "summary_token_frac": 0.15552099533437014, "baseline_indices": [61, 5, 4, 6, 17, 28, 35, 62, 7, 20, 14, 9, 59, 2, 23, 13, 21, 24, 30, 11, 58, 27, 8, 12], "raptor_node_indices": [13, 41, 44, 39, 11, 6, 8, 28, 45, 40, 10, 15, 48, 56, 9, 58, 14, 65, 62, 68, 38, 27], "raptor_leaf_indices": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 38, 39, 40, 41, 44, 45, 48, 56, 57, 58], "baseline_snippets": ["Finally, we demonstrated use of our models in applications, finding associations between demographics and rate of mentioning causal explanations, as well as showing differences in the top words predictive of negative ratings in Yelp reviews  Utilization of discourse structure in social media analysis has been a largely untapped area of exploration, perhaps due to its perceived difficulty", "Prevailing approaches for social media analyses, utilizing traditional linear models or bag of words models (e g , SVM trained with n-gram, part-of-speech (POS) tags, or lexicon-based features) alone do not seem appropriate for this task since they simply cannot segment the text into meaningful discourse units or discourse arguments such as clauses or sentences rather than random consecutive token sequences or specific word tokens", " First, the ungrammatical texts in social media incur poor syntactic parsing results which drastically affect the performance of discourse relation parsing pipelines   Many causal relations are implicit and do not contain any discourse markers (e g , `because')  Further, Explicit causal relations are also more difficult in social media due to the abundance of abbreviations and variations of discourse connectives (e g , `cuz' and `bcuz')"], "raptor_snippets": [" Even when the discourse units are clear, parsers may still fail to accurately identify discourse relations since the content of social media is quite different than that of newswire which is typically used for discourse parsing In order to overcome these difficulties of discourse relation parsing in social media, we simplify and minimize the use of syntactic parsing results and capture relations between discourse arguments, and investigate the use of a recursive neural network model (RNN)", " Recent work has shown that RNNs are effective for utilizing discourse structures for their downstream tasks BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , but they have yet to be directly used for discourse relation prediction in social media  We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models", "The contributions of this work include (1) the proposal of models for both (a) causality prediction and (b) causal explanation identification (2) the extensive evaluation of a variety of models from social media classification models and discourse relation parsers to RNN-based application models demonstrating that feature-based models work best for causality prediction while RNNs are superior for the more difficult task of causal explanation identification (3) performance analysis on architectural differences of the pipeline and the classifier structures"]}
